# Algorithm to use. Supported values: "PPO".
algorithm.name = "PPO"

# The reward is not delayed based on how the environment works; it is the direct
# result of the previous action. Therefore, we can use lower gamma and lambda
# hyperparameter values.
algorithm.gamma = 0.7
algorithm.lambda = 0.3

# Do not use the GPU to train neural networks.
disable_gpu = false

# Number of training iterations to run.
iterations = 100

# Number of episodes to play for each training iteration.
#
# This number must be divisible by the number of runners (all runners must play
# the same number of episodes).
training_num_episodes = 1

# The number of parallel runners that will be launched to collect experiences in
# a training iteration. If the number is 0, the master process will collect
# experiences.
runners = 1

# Model configuration.
model = "configs/models/softplus.json"

# Base seed for the various RNGs used in the experiment.
seed = 42

# Create a checkpoint after the specified number of iterations. If the number is
# 0, then no checkpoints are created except for the final one.
checkpoint_interval = 25

# Run an evaluation after the specified number of iterations. If the number is
# 0, then no evaluation is done except for the final one.
evaluation_interval = 25

evaluation_num_episodes = 10

# Disable the final evaluation.
final_evaluation = true
