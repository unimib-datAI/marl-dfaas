# This is the default configuration file used for running experiments.

# Algorithm to use. Supported values: "PPO".
algorithm.name = "PPO"

# Gamma is the discount factor. It defines the weights of future rewards when
# calculating the return, which is the cumulative reward that an agent learns to
# maximize.
#
# When gamma is closer to 0, the agent considers only immediate rewards. When
# gamma is closer to 1, the agent considers future rewards to be as important as
# immediate ones.
algorithm.gamma = 0.99

# Do not use the GPU to train neural networks.
disable_gpu = false

# Number of training iterations to run.
iterations = 100

# Number of episodes to play for each training iteration.
#
# This number must be divisible by the number of runners (all runners must play
# the same number of episodes).
training_num_episodes = 3

# The number of parallel runners that will be launched to collect experiences in
# a training iteration. If the number is 0, the master process will collect
# experiences.
runners = 3

# Model configuration.
model = "configs/models/softplus.json"

# Base seed for the various RNGs used in the experiment.
seed = 42

# Create a checkpoint after the specified number of iterations. If the number is
# 0, then no checkpoints are created except for the final one.
checkpoint_interval = 25

# Run an evaluation after the specified number of iterations. If the number is
# 0, then no evaluation is done except for the final one.
evaluation_interval = 25

# Number of episodes to play for each evaluation interval.
evaluation_num_episodes = 10

# Disable the final evaluation.
final_evaluation = true

[[policies]]
name = "policy_node_0"
class = "APLPolicy"

[[policies]]
name = "policy_node_1"
# Use default policy (PPO) (class = None).
