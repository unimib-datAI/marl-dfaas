{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Evaluation summary of a single experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook contains plots and analyses of evaluations performed during an experiment's training process. Note that the evaluation is done after some training iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Experiment loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports.\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib widget\n",
    "import base\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import dfaas_env\n",
    "import dfaas_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_dir = Path(\"/home/emanuele/marl-dfaas/results/\")\n",
    "\n",
    "exp_dir = prefix_dir / \"DFAAS-MA_2025-05-21_17-45-58_PPO_constant_rate_det_5_100\"\n",
    "\n",
    "evaluation_data = dfaas_utils.json_to_dict(exp_dir / \"evaluation.json\")\n",
    "\n",
    "# Reference environment.\n",
    "env = base.get_env(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Experiment prefix dir: {prefix_dir.as_posix()!r}\")\n",
    "print(f\"Experiment name:       {exp_dir.name!r}\")\n",
    "print(f\"Agents:                {env.agents} ({len(env.agents)})\")\n",
    "print(f\"Evaluations:           {len(evaluation_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_data_avg(eval_data, env):\n",
    "    \"\"\"Returns the average/min/max cumulative reward per evaluation for each agent and all agents.\"\"\"\n",
    "    episodes_per_eval = eval_data[0][\"env_runners\"][\"num_episodes\"]\n",
    "\n",
    "    reward_avg = {\"mean\": {}, \"min\": {}, \"max\": {}}\n",
    "    for key in reward_avg:\n",
    "        reward_avg[key][\"all\"] = np.empty(len(eval_data))\n",
    "        for agent in env.agents:\n",
    "            reward_avg[key][agent] = np.empty_like(reward_avg[key][\"all\"])\n",
    "\n",
    "    for eval_idx in range(len(eval_data)):\n",
    "        assert (\n",
    "            eval_data[eval_idx][\"env_runners\"][\"episodes_this_iter\"] == episodes_per_eval\n",
    "        ), \"Episodes per eval must be the same for all evaluations!\"\n",
    "\n",
    "        for agent in env.agents:\n",
    "            policy_name = f\"policy_{agent}\"\n",
    "\n",
    "            reward_avg[\"mean\"][agent][eval_idx] = eval_data[eval_idx][\"env_runners\"][\"policy_reward_mean\"][policy_name]\n",
    "            reward_avg[\"min\"][agent][eval_idx] = eval_data[eval_idx][\"env_runners\"][\"policy_reward_min\"][policy_name]\n",
    "            reward_avg[\"max\"][agent][eval_idx] = eval_data[eval_idx][\"env_runners\"][\"policy_reward_max\"][policy_name]\n",
    "\n",
    "        reward_avg[\"mean\"][\"all\"][eval_idx] = eval_data[eval_idx][\"env_runners\"][\"episode_reward_mean\"]\n",
    "        reward_avg[\"min\"][\"all\"][eval_idx] = eval_data[eval_idx][\"env_runners\"][\"episode_reward_min\"]\n",
    "        reward_avg[\"max\"][\"all\"][eval_idx] = eval_data[eval_idx][\"env_runners\"][\"episode_reward_max\"]\n",
    "\n",
    "    return reward_avg\n",
    "\n",
    "\n",
    "reward_data_avg = get_reward_data_avg(evaluation_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cumulative_reward_plot(data, env):\n",
    "    for agent in [\"all\"] + env.agents:\n",
    "        plt.close(fig=f\"reward_cum_{agent}\")\n",
    "        fig = plt.figure(num=f\"reward_cum_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        reward = data[\"mean\"][agent]\n",
    "        reward_min = data[\"min\"][agent]\n",
    "        reward_max = data[\"max\"][agent]\n",
    "\n",
    "        eval_steps = np.arange(len(reward))\n",
    "\n",
    "        smoothed = savgol_filter(reward, 15, 3)  # window size 15, polynomial order 3\n",
    "\n",
    "        ax.plot(eval_steps, reward, label=\"Original\")\n",
    "        ax.plot(eval_steps, smoothed, label=\"Smoothed\")\n",
    "\n",
    "        plt.fill_between(eval_steps, reward_min, reward_max, alpha=0.2, color=\"blue\", label=\"Min/Max\")\n",
    "\n",
    "        ax.set_title(f\"Average cumulative reward per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Reward\")\n",
    "        ax.set_xlabel(\"Evaluation\")\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_cumulative_reward_plot(reward_data_avg, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Agent rejection rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_reject_rate(eval_data, env):\n",
    "    \"\"\"Returns the average/min/max agent rejection rate per evaluation for each agent and all agents. Returns two results: one with absolute values and one with ratios over the input rate.\n",
    "\n",
    "    Two notes:\n",
    "        1. It is an average because multiple episodes are played for each evaluation.\n",
    "\n",
    "        2. The agent rejection rate is the sum of the agent rejection rate, the locally routed rate that was rejected, and the forwarded rate that was rejected.\n",
    "    \"\"\"\n",
    "    episodes_per_eval = eval_data[0][\"env_runners\"][\"num_episodes\"]\n",
    "\n",
    "    reject_avg = {\"mean\": {}, \"min\": {}, \"max\": {}}\n",
    "    reject_ratio = {\"mean\": {}, \"min\": {}, \"max\": {}}\n",
    "    for key in reject_avg:\n",
    "        reject_avg[key][\"all\"] = np.empty(len(eval_data))\n",
    "        reject_ratio[key][\"all\"] = np.empty(len(eval_data))\n",
    "        for agent in env.agents:\n",
    "            reject_avg[key][agent] = np.empty_like(reject_avg[key][\"all\"])\n",
    "            reject_ratio[key][agent] = np.empty_like(reject_ratio[key][\"all\"])\n",
    "\n",
    "    for eval_idx in range(len(eval_data)):\n",
    "        assert (\n",
    "            eval_data[eval_idx][\"env_runners\"][\"episodes_this_iter\"] == episodes_per_eval\n",
    "        ), \"Episodes per eval must be the same for all evaluations!\"\n",
    "\n",
    "        reject_epi = {agent: np.zeros(episodes_per_eval) for agent in [\"all\"] + env.agents}\n",
    "        input_epi = {agent: np.zeros(episodes_per_eval) for agent in [\"all\"] + env.agents}\n",
    "        for epi_idx in range(episodes_per_eval):\n",
    "            for agent in env.agents:\n",
    "                input_rate = sum(\n",
    "                    eval_data[eval_idx][\"env_runners\"][\"hist_stats\"][\"observation_input_requests\"][epi_idx][agent]\n",
    "                )\n",
    "                input_epi[agent][epi_idx] = input_rate\n",
    "                input_epi[\"all\"][epi_idx] += input_rate\n",
    "\n",
    "                action_reject = sum(eval_data[eval_idx][\"env_runners\"][\"hist_stats\"][\"action_reject\"][epi_idx][agent])\n",
    "                local_reject = sum(\n",
    "                    eval_data[eval_idx][\"env_runners\"][\"hist_stats\"][\"incoming_rate_local_reject\"][epi_idx][agent]\n",
    "                )\n",
    "                forward_reject = sum(\n",
    "                    eval_data[eval_idx][\"env_runners\"][\"hist_stats\"][\"forward_reject_rate\"][epi_idx][agent]\n",
    "                )\n",
    "\n",
    "                reject_rate = action_reject + local_reject + forward_reject\n",
    "                reject_epi[agent][epi_idx] = reject_rate\n",
    "                reject_epi[\"all\"][epi_idx] += reject_rate\n",
    "\n",
    "        for agent in [\"all\"] + env.agents:\n",
    "            reject_avg[\"mean\"][agent][eval_idx] = np.average(reject_epi[agent])\n",
    "            reject_avg[\"min\"][agent][eval_idx] = np.min(reject_epi[agent])\n",
    "            reject_avg[\"max\"][agent][eval_idx] = np.max(reject_epi[agent])\n",
    "\n",
    "            reject_ratio_eval = reject_epi[agent] / input_epi[agent]\n",
    "            reject_ratio[\"mean\"][agent][eval_idx] = np.average(reject_ratio_eval)\n",
    "            reject_ratio[\"min\"][agent][eval_idx] = np.min(reject_ratio_eval)\n",
    "            reject_ratio[\"max\"][agent][eval_idx] = np.max(reject_ratio_eval)\n",
    "\n",
    "    return reject_avg, reject_ratio\n",
    "\n",
    "\n",
    "agent_reject_rate_abs, agent_reject_rate_ratio = get_agent_reject_rate(evaluation_data, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_average_agent_reject_rate_abs(data, env):\n",
    "    for agent in [\"all\"] + env.agents:\n",
    "        plt.close(fig=f\"average_agent_reject_rate_abs_{agent}\")\n",
    "        fig = plt.figure(num=f\"average_agent_reject_rate_abs_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        reject = data[\"mean\"][agent]\n",
    "        reject_min = data[\"min\"][agent]\n",
    "        reject_max = data[\"max\"][agent]\n",
    "\n",
    "        eval_steps = np.arange(len(reject))\n",
    "\n",
    "        smoothed = savgol_filter(reject, 15, 3)  # window size 15, polynomial order 3\n",
    "\n",
    "        ax.plot(reject, label=\"Original\")\n",
    "        ax.plot(smoothed, label=\"Smoothed\")\n",
    "\n",
    "        plt.fill_between(eval_steps, reject_min, reject_max, alpha=0.2, color=\"blue\", label=\"Min/Max\")\n",
    "\n",
    "        ax.set_title(f\"Average agent reject¹ rate ({agent = })\\n(cumulative value per episode)\")\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.2,\n",
    "            \"¹reject rate = action reject + local reject + forward reject\",\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        ax.set_ylabel(\"Reject rate\")\n",
    "        ax.set_xlabel(\"Evaluation\")\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_average_agent_reject_rate_abs(agent_reject_rate_abs, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Percent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_average_agent_reject_rate_perc(data, env):\n",
    "    for agent in [\"all\"] + env.agents:\n",
    "        plt.close(fig=f\"average_agent_reject_rate_perc_{agent}\")\n",
    "        fig = plt.figure(num=f\"average_agent_reject_rate_perc_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        reject = data[\"mean\"][agent] * 100\n",
    "        reject_min = data[\"min\"][agent] * 100\n",
    "        reject_max = data[\"max\"][agent] * 100\n",
    "\n",
    "        eval_steps = np.arange(len(reject))\n",
    "\n",
    "        smoothed = savgol_filter(reject, 15, 3)  # window size 15, polynomial order 3\n",
    "\n",
    "        ax.plot(reject, label=\"Original\")\n",
    "        ax.plot(smoothed, label=\"Smoothed\")\n",
    "\n",
    "        plt.fill_between(eval_steps, reject_min, reject_max, alpha=0.2, color=\"blue\", label=\"Min/Max\")\n",
    "\n",
    "        ax.set_title(f\"Average agent reject¹ rate ({agent = })\\n(cumulative % value over the input rate per episode)\")\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.2,\n",
    "            \"¹reject rate = action reject + local reject + forward reject\",\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        ax.set_ylabel(\"Reject rate\")\n",
    "        ax.yaxis.set_major_formatter(ticker.PercentFormatter())\n",
    "        ax.set_xlabel(\"Evaluation\")\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_average_agent_reject_rate_perc(agent_reject_rate_ratio, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
