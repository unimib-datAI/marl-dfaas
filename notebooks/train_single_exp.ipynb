{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Train summary of a single experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook is for the experiment with PPO and one episode played for iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Experiment loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports.\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib widget\n",
    "import base\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import dfaas_env\n",
    "import dfaas_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_dir = Path(\"/home/emanuele/marl-dfaas/results/\")\n",
    "\n",
    "exp_dir = prefix_dir / \"DF_20250612_163451_PPO_two_agents_constant_3000_no_exp\"\n",
    "\n",
    "# Raw data dictionary \"result.json\".\n",
    "raw_exp_data = dfaas_utils.parse_result_file(exp_dir / \"result.json\")\n",
    "\n",
    "# Reference environment.\n",
    "env = base.get_env(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Experiment prefix dir: {prefix_dir.as_posix()!r}\")\n",
    "print(f\"Experiment name:       {exp_dir.name!r}\")\n",
    "print(f\"Agents:                {env.agents} ({len(env.agents)})\")\n",
    "print(f\"Iterations:            {len(raw_exp_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_data_step(raw_exp_data, env):\n",
    "    \"\"\"Returns the average reward per step for each agent and all agents.\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reward_step = {}\n",
    "    reward_step[\"all\"] = np.empty(iters_n)\n",
    "    for agent in env.agents:\n",
    "        reward_step[agent] = np.empty(iters_n)\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        episodes = raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"]\n",
    "        assert episodes > 0\n",
    "\n",
    "        agent_rewards = {agent: [] for agent in env.agents}\n",
    "        for epi in range(episodes):\n",
    "            for agent in env.agents:\n",
    "                agent_rewards[agent].extend(raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"reward\"][epi][agent])\n",
    "\n",
    "        reward_step_tmp = []\n",
    "        for agent in env.agents:\n",
    "            # Average over all steps from all episodes\n",
    "            reward_step[agent][iter_idx] = np.average(agent_rewards[agent])\n",
    "            reward_step_tmp.append(reward_step[agent][iter_idx])\n",
    "\n",
    "        reward_step[\"all\"][iter_idx] = np.average(reward_step_tmp)\n",
    "\n",
    "    return reward_step\n",
    "\n",
    "\n",
    "def get_reward_data_sum(raw_exp_data, env):\n",
    "    \"\"\"Returns the cumulative reward per episode for each agent and all agents.\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reward_sum = {agent: np.empty(iters_n) for agent in env.agents}\n",
    "    reward_sum[\"all\"] = np.empty(iters_n)\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        episodes = raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"]\n",
    "        assert episodes > 0\n",
    "\n",
    "        reward_iter = {agent: np.empty(episodes) for agent in env.agents}\n",
    "        reward_iter[\"all\"] = np.empty(episodes)\n",
    "        for epi in range(episodes):\n",
    "            reward_iter[\"all\"][epi] = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"episode_reward\"][epi]\n",
    "            for agent in env.agents:\n",
    "                reward_iter[agent][epi] = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\n",
    "                    f\"policy_policy_{agent}_reward\"\n",
    "                ][epi]\n",
    "\n",
    "        reward_sum[\"all\"][iter_idx] = np.average(reward_iter[\"all\"])\n",
    "        for agent in env.agents:\n",
    "            reward_sum[agent][iter_idx] = np.average(reward_iter[agent])\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "reward_sum = get_reward_data_sum(raw_exp_data, env)\n",
    "reward_step = get_reward_data_step(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Cumulative reward per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cumulative_reward_plot(reward_sum):\n",
    "    for agent, reward in sorted(reward_sum.items()):\n",
    "        plt.close(fig=f\"reward_cum_{agent}\")\n",
    "        fig = plt.figure(num=f\"reward_cum_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(reward)\n",
    "\n",
    "        ax.set_title(f\"Average cumulative reward per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Reward\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_cumulative_reward_plot(reward_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Average reward per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_average_reward_step_plot(reward_step):\n",
    "    for agent, reward in reward_step.items():\n",
    "        plt.close(fig=f\"reward_step_{agent}\")\n",
    "        fig = plt.figure(num=f\"reward_step_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(reward)\n",
    "\n",
    "        ax.set_title(f\"Average reward per step per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Reward\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_average_reward_step_plot(reward_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Action distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_action_iter(raw_exp_data, iter_idx, env, agent):\n",
    "    episodes = raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"]\n",
    "    assert episodes > 0\n",
    "\n",
    "    action_dist_iter = {\"local\": np.zeros(episodes), \"forward\": np.zeros(episodes), \"reject\": np.zeros(episodes)}\n",
    "\n",
    "    for epi in range(episodes):\n",
    "        action_dist_step = {\n",
    "            \"local\": np.zeros(env.max_steps),\n",
    "            \"forward\": np.zeros(env.max_steps),\n",
    "            \"reject\": np.zeros(env.max_steps),\n",
    "        }\n",
    "\n",
    "        # Before calculating the average, we need to normalize the steps.\n",
    "        for step in range(env.max_steps):\n",
    "            local = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"action_local\"][epi][agent][step]\n",
    "            forward = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"action_forward\"][epi][agent][step]\n",
    "            reject = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"action_reject\"][epi][agent][step]\n",
    "\n",
    "            action_sum = local + forward + reject\n",
    "            assert action_sum > 0\n",
    "\n",
    "            action_dist_step[\"local\"] = local / action_sum\n",
    "            action_dist_step[\"forward\"] = forward / action_sum\n",
    "            action_dist_step[\"reject\"] = reject / action_sum\n",
    "\n",
    "        action_dist_iter[\"local\"][epi] = np.average(action_dist_step[\"local\"])\n",
    "        action_dist_iter[\"forward\"][epi] = np.average(action_dist_step[\"forward\"])\n",
    "        action_dist_iter[\"reject\"][epi] = np.average(action_dist_step[\"reject\"])\n",
    "\n",
    "    return action_dist_iter\n",
    "\n",
    "\n",
    "def get_action_distribution(raw_exp_data, env):\n",
    "    \"\"\"Returns the average action distribution (normalized to 1) for each iteration for all agents.\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    action_dist = {}\n",
    "    for agent in [\"all\"] + env.agents:\n",
    "        action_dist[agent] = {}\n",
    "        action_dist[agent][\"local\"] = np.empty(iters_n)\n",
    "        action_dist[agent][\"forward\"] = np.empty(iters_n)\n",
    "        action_dist[agent][\"reject\"] = np.empty(iters_n)\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        action_dist_tmp = {}\n",
    "        action_dist_tmp[\"local\"] = []\n",
    "        action_dist_tmp[\"forward\"] = []\n",
    "        action_dist_tmp[\"reject\"] = []\n",
    "\n",
    "        for agent in env.agents:\n",
    "            if agent == \"all\":\n",
    "                continue\n",
    "\n",
    "            action_dist_iter = get_normalized_action_iter(raw_exp_data, iter_idx, env, agent)\n",
    "\n",
    "            action_dist[agent][\"local\"][iter_idx] = np.average(action_dist_iter[\"local\"])\n",
    "            action_dist[agent][\"forward\"][iter_idx] = np.average(action_dist_iter[\"forward\"])\n",
    "            action_dist[agent][\"reject\"][iter_idx] = np.average(action_dist_iter[\"reject\"])\n",
    "\n",
    "            action_dist_tmp[\"local\"].append(action_dist[agent][\"local\"][iter_idx])\n",
    "            action_dist_tmp[\"forward\"].append(action_dist[agent][\"forward\"][iter_idx])\n",
    "            action_dist_tmp[\"reject\"].append(action_dist[agent][\"reject\"][iter_idx])\n",
    "\n",
    "        action_dist[\"all\"][\"local\"][iter_idx] = np.average(action_dist_tmp[\"local\"])\n",
    "        action_dist[\"all\"][\"forward\"][iter_idx] = np.average(action_dist_tmp[\"forward\"])\n",
    "        action_dist[\"all\"][\"reject\"][iter_idx] = np.average(action_dist_tmp[\"reject\"])\n",
    "\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "action_dist = get_action_distribution(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_action_distribution_plot(action_dist):\n",
    "    for agent, dist in action_dist.items():\n",
    "        plt.close(fig=f\"action_dist_{agent}\")\n",
    "        fig = plt.figure(num=f\"action_dist_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        steps = np.arange(len(raw_exp_data))\n",
    "\n",
    "        ax.stackplot(\n",
    "            steps, dist[\"local\"], dist[\"forward\"], dist[\"reject\"], labels=[\"Local\", \"Forward\", \"Reject\"], alpha=0.8\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Average action distribution per step per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Action proportion\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_action_distribution_plot(action_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Rejections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Differences between reject rates:\n",
    "\n",
    "* **Node reject rate**: it considers the reject rate of the incoming local rate and the _incoming forwarded rate from the neighbors_.\n",
    "* **Agent reject rate**: it is the sum of reject rate decide by the agent (`action_reject`), the rejected incoming local rate (`incoming_rate_reject`) and the rejected forwarded rate (`forward_reject_rate`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Node reject rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reject_data(raw_exp_data, env):\n",
    "    \"\"\"Returns the average node reject rate for each step for each iteration for all agents (including \"all\" agents, which is the sum of all agents).\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reject_data = {agent: np.empty(iters_n) for agent in env.agents}\n",
    "    reject_data[\"all\"] = np.empty(iters_n)\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        episodes = raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"]\n",
    "        assert episodes > 0\n",
    "\n",
    "        average_reject_iter = {agent: np.empty(episodes) for agent in env.agents}\n",
    "        for epi in range(episodes):\n",
    "            for agent in env.agents:\n",
    "                percent_reject = np.zeros(env.max_steps)\n",
    "                for step in range(env.max_steps):\n",
    "                    incoming_rate = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"incoming_rate\"][epi][agent][\n",
    "                        step\n",
    "                    ]\n",
    "                    incoming_rate_reject = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"incoming_rate_reject\"][\n",
    "                        epi\n",
    "                    ][agent][step]\n",
    "\n",
    "                    if incoming_rate == 0:\n",
    "                        continue  # The agent has forwarded/rejected all requests.\n",
    "\n",
    "                    percent_reject[step] = incoming_rate_reject / incoming_rate * 100\n",
    "\n",
    "                average_reject_iter[agent][epi] = np.average(percent_reject)\n",
    "\n",
    "        reject_data_tmp = []\n",
    "        for agent in env.agents:\n",
    "            reject_data[agent][iter_idx] = np.average(average_reject_iter[agent])\n",
    "            reject_data_tmp.append(reject_data[agent][iter_idx])\n",
    "        reject_data[\"all\"][iter_idx] = np.average(reject_data_tmp)\n",
    "\n",
    "    return reject_data\n",
    "\n",
    "\n",
    "reject_data = get_reject_data(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_average_reject_step_plot(reject_data):\n",
    "    for agent, reject in sorted(reject_data.items()):\n",
    "        plt.close(fig=f\"average_reject_step_{agent}\")\n",
    "        fig = plt.figure(num=f\"average_reject_step_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(reject)\n",
    "\n",
    "        ax.set_title(f\"Average node reject rate ({agent = })\\n(average node reject rate¹ per step per episode)\")\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.2,\n",
    "            \"¹reject rate = local reject rate + incoming forwarded reject rate\",\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        ax.set_ylabel(\"Reject rate\")\n",
    "        ax.yaxis.set_major_formatter(ticker.PercentFormatter())\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_average_reject_step_plot(reject_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Agent reject rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_reject_rate(raw_exp_data, env):\n",
    "    \"\"\"Returns the average agent reject rate for each step for each iteration for all agents (including \"all\" agents, which is the sum of all agents).\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reject_data = {agent: np.empty(iters_n) for agent in env.agents}\n",
    "    reject_data[\"all\"] = np.empty(iters_n)\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        episodes = raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"]\n",
    "        assert episodes > 0\n",
    "\n",
    "        average_reject_iter = {agent: np.empty(episodes) for agent in env.agents}\n",
    "        for epi in range(episodes):\n",
    "            for agent in env.agents:\n",
    "                percent_reject = np.zeros(env.max_steps)\n",
    "                for step in range(env.max_steps):\n",
    "                    input_rate = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"observation_input_rate\"][epi][\n",
    "                        agent\n",
    "                    ][step]\n",
    "\n",
    "                    action_reject = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"action_reject\"][epi][agent][\n",
    "                        step\n",
    "                    ]\n",
    "                    local_reject = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"incoming_rate_local_reject\"][\n",
    "                        epi\n",
    "                    ][agent][step]\n",
    "                    forward_reject = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"forward_reject_rate\"][epi][\n",
    "                        agent\n",
    "                    ][step]\n",
    "                    reject_rate = action_reject + local_reject + forward_reject\n",
    "\n",
    "                    assert input_rate > 0\n",
    "\n",
    "                    percent_reject[step] = reject_rate / input_rate * 100\n",
    "\n",
    "                average_reject_iter[agent][epi] = np.average(percent_reject)\n",
    "\n",
    "        reject_data_tmp = []\n",
    "        for agent in env.agents:\n",
    "            reject_data[agent][iter_idx] = np.average(average_reject_iter[agent])\n",
    "            reject_data_tmp.append(reject_data[agent][iter_idx])\n",
    "        reject_data[\"all\"][iter_idx] = np.average(reject_data_tmp)\n",
    "\n",
    "    return reject_data\n",
    "\n",
    "\n",
    "agent_reject_rate = get_agent_reject_rate(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_average_agent_reject_step_plot(reject_data):\n",
    "    for agent, reject in sorted(reject_data.items()):\n",
    "        plt.close(fig=f\"average_agent_reject_step_{agent}\")\n",
    "        fig = plt.figure(num=f\"average_agent_reject_step_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(reject)\n",
    "\n",
    "        ax.set_title(f\"Average agent reject rate ({agent = })\\n(average agent reject rate¹ per step per episode)\")\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.2,\n",
    "            \"¹reject rate = action reject + local reject + forward reject\",\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        ax.set_ylabel(\"Reject rate\")\n",
    "        ax.yaxis.set_major_formatter(ticker.PercentFormatter())\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_average_agent_reject_step_plot(agent_reject_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Rejections by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reject_rate_dist_by_type(raw_exp_data, env):\n",
    "    \"\"\"Returns the reject rate distribution by type as a ratio to the total reject rate for each step, for each iteration, and for all agents (where \"all\" agent is just the sum of all agent values). Each value is the average for one iteration (= one episode).\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reject_types = [\"excess_local\", \"action\", \"excess_forward\"]\n",
    "\n",
    "    # The output dictionary has two level of nesting: agent -> reject type -> values.\n",
    "    reject_data = {\n",
    "        agent: {reject_type: np.zeros(iters_n) for reject_type in reject_types} for agent in [\"all\"] + env.agents\n",
    "    }\n",
    "\n",
    "    for iter_idx in range(iters_n):\n",
    "        episodes = raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"]\n",
    "        assert episodes > 0\n",
    "\n",
    "        # A temporary dictionary used to collect data for this iteration.\n",
    "        reject_data_iter = {\n",
    "            agent: {reject_type: np.zeros(episodes) for reject_type in reject_types} for agent in [\"all\"] + env.agents\n",
    "        }\n",
    "        for epi in range(episodes):\n",
    "            action_reject_tmp, local_reject_tmp, forward_reject_tmp = [], [], []\n",
    "            for agent in env.agents:\n",
    "                # Reject rate arrays (each index is one step).\n",
    "                action_reject = np.array(\n",
    "                    raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"action_reject\"][epi][agent]\n",
    "                )\n",
    "                local_reject = np.array(\n",
    "                    raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"incoming_rate_local_reject\"][epi][agent]\n",
    "                )\n",
    "                forward_reject = np.array(\n",
    "                    raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"forward_reject_rate\"][epi][agent]\n",
    "                )\n",
    "                total_reject = action_reject + local_reject + forward_reject\n",
    "\n",
    "                if total_reject.sum() > 0:\n",
    "                    reject_data_iter[agent][\"action\"][epi] = action_reject.sum() / total_reject.sum()\n",
    "                    reject_data_iter[agent][\"excess_local\"][epi] = local_reject.sum() / total_reject.sum()\n",
    "                    reject_data_iter[agent][\"excess_forward\"][epi] = forward_reject.sum() / total_reject.sum()\n",
    "\n",
    "                action_reject_tmp.append(reject_data_iter[agent][\"action\"][epi])\n",
    "                local_reject_tmp.append(reject_data_iter[agent][\"excess_local\"][epi])\n",
    "                forward_reject_tmp.append(reject_data_iter[agent][\"excess_forward\"][epi])\n",
    "\n",
    "            # Calculate the average distribution for the \"all\" agent.\n",
    "            action_reject = np.array(action_reject_tmp)\n",
    "            local_reject = np.array(local_reject_tmp)\n",
    "            forward_reject = np.array(forward_reject_tmp)\n",
    "            total_reject = action_reject + local_reject + forward_reject\n",
    "            if total_reject.sum() > 0:\n",
    "                reject_data_iter[\"all\"][\"action\"][epi] = action_reject.sum() / total_reject.sum()\n",
    "                reject_data_iter[\"all\"][\"excess_local\"][epi] = local_reject.sum() / total_reject.sum()\n",
    "                reject_data_iter[\"all\"][\"excess_forward\"][epi] = forward_reject.sum() / total_reject.sum()\n",
    "\n",
    "        # Average the statistics for a single iteration.\n",
    "        for agent in reject_data_iter.keys():\n",
    "            for reject_type in reject_data_iter[agent].keys():\n",
    "                reject_data[agent][reject_type][iter_idx] = np.average(reject_data_iter[agent][reject_type])\n",
    "\n",
    "    return reject_data\n",
    "\n",
    "\n",
    "reject_rate_dist_by_type = get_reject_rate_dist_by_type(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reject_type_distribution_plot(reject_rate_dist_by_type):\n",
    "    for agent, dist in reject_rate_dist_by_type.items():\n",
    "        plt.close(fig=f\"reject_type_distribution_{agent}\")\n",
    "        fig = plt.figure(num=f\"reject_type_distribution_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        iter_n = np.arange(len(raw_exp_data))\n",
    "\n",
    "        ax.stackplot(\n",
    "            iter_n,\n",
    "            dist[\"action\"],\n",
    "            dist[\"excess_local\"],\n",
    "            dist[\"excess_forward\"],\n",
    "            labels=[\"By action\", \"Excess local\", \"Excess forward\"],\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Average reject type per step per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Reject distribution\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_reject_type_distribution_plot(reject_rate_dist_by_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
