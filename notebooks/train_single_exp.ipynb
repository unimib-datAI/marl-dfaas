{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Train summary of a single experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook is for the experiment with PPO and one episode played for iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Experiment loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports.\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib widget\n",
    "import base\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import dfaas_env\n",
    "import dfaas_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_dir = Path(\"/home/emanuele/marl-dfaas/results/\")\n",
    "\n",
    "exp_dir = prefix_dir / \"DFAAS-MA_2025-05-13_10-51-39_PPO_2_constant_rate\"\n",
    "\n",
    "# Raw data dictionary \"result.json\".\n",
    "raw_exp_data = dfaas_utils.parse_result_file(exp_dir / \"result.json\")\n",
    "\n",
    "# Reference environment.\n",
    "env = base.get_env(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Experiment prefix dir: {prefix_dir.as_posix()!r}\")\n",
    "print(f\"Experiment name:       {exp_dir.name!r}\")\n",
    "print(f\"Agents:                {env.agents} ({len(env.agents)})\")\n",
    "print(f\"Iterations:            {len(raw_exp_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_data_step(raw_exp_data, env):\n",
    "    \"\"\"Returns the average reward per step for each agent and all agents.\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reward_step = {}\n",
    "    reward_step[\"all\"] = np.empty(iters_n)\n",
    "    for agent in env.agents:\n",
    "        reward_step[agent] = np.empty(iters_n)\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        assert (\n",
    "            raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"] == 1\n",
    "        ), \"Only iterations with one episode are supported!\"\n",
    "\n",
    "        reward_step_tmp = []\n",
    "        for agent in env.agents:\n",
    "            reward_step[agent][iter_idx] = np.average(\n",
    "                raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"reward\"][0][agent]\n",
    "            )\n",
    "            reward_step_tmp.append(reward_step[agent][iter_idx])\n",
    "\n",
    "        reward_step[\"all\"][iter_idx] = np.average(reward_step_tmp)\n",
    "\n",
    "    return reward_step\n",
    "\n",
    "\n",
    "def get_reward_data_sum(raw_exp_data, env):\n",
    "    \"\"\"Returns the cumulative reward per episode for each agent and all agents.\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reward_sum = {}\n",
    "    reward_sum[\"all\"] = np.empty(iters_n)\n",
    "    for agent in env.agents:\n",
    "        reward_sum[agent] = np.empty(iters_n)\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        assert (\n",
    "            raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"] == 1\n",
    "        ), \"Only iterations with one episode are supported!\"\n",
    "\n",
    "        reward_sum[\"all\"][iter_idx] = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"episode_reward\"][0]\n",
    "        for agent in env.agents:\n",
    "            reward_sum[agent][iter_idx] = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\n",
    "                f\"policy_policy_{agent}_reward\"\n",
    "            ][0]\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "reward_sum = get_reward_data_sum(raw_exp_data, env)\n",
    "reward_step = get_reward_data_step(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Cumulative reward per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Since we play one episode per iteration, the cumulative reward per episode is just the sum of the individual rewards for each step (and for each agent). The special case \"all\" agents is the sum of all agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cumulative_reward_plot(reward_sum):\n",
    "    for agent, reward in reward_sum.items():\n",
    "        plt.close(fig=f\"reward_cum_{agent}\")\n",
    "        fig = plt.figure(num=f\"reward_cum_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        smoothed = savgol_filter(reward, 15, 3)  # window size 15, polynomial order 3\n",
    "\n",
    "        ax.plot(reward, label=\"Original\")\n",
    "        ax.plot(smoothed, label=\"Smoothed\")\n",
    "\n",
    "        ax.set_title(f\"Cumulative reward per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Reward\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(25))  # Show x-axis ticks every 50 iterations.\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_cumulative_reward_plot(reward_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Average reward per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_average_reward_step_plot(reward_step):\n",
    "    for agent, reward in reward_step.items():\n",
    "        plt.close(fig=f\"reward_step_{agent}\")\n",
    "        fig = plt.figure(num=f\"reward_step_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        smoothed = savgol_filter(reward, 15, 3)  # window size 15, polynomial order 3\n",
    "\n",
    "        ax.plot(reward, label=\"Original\")\n",
    "        ax.plot(smoothed, label=\"Smoothed\")\n",
    "\n",
    "        ax.set_title(f\"Average reward per step per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Reward\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(25))  # Show x-axis ticks every 50 iterations.\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_average_reward_step_plot(reward_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Action distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_action_step(raw_exp_data, iter_idx, env, agent):\n",
    "    action_dist_step = {}\n",
    "    action_dist_step[\"local\"] = np.zeros(env.max_steps)\n",
    "    action_dist_step[\"forward\"] = np.zeros(env.max_steps)\n",
    "    action_dist_step[\"reject\"] = np.zeros(env.max_steps)\n",
    "\n",
    "    # Before calculating the average, we need to normalize the steps.\n",
    "    for step in range(env.max_steps):\n",
    "        local = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"action_local\"][0][agent][step]\n",
    "        forward = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"action_forward\"][0][agent][step]\n",
    "        reject = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"action_reject\"][0][agent][step]\n",
    "\n",
    "        action_sum = local + forward + reject\n",
    "        if action_sum == 0:\n",
    "            continue\n",
    "\n",
    "        action_dist_step[\"local\"] = local / action_sum\n",
    "        action_dist_step[\"forward\"] = forward / action_sum\n",
    "        action_dist_step[\"reject\"] = reject / action_sum\n",
    "\n",
    "    return action_dist_step\n",
    "\n",
    "\n",
    "def get_action_distribution(raw_exp_data, env):\n",
    "    \"\"\"Returns the average action distribution (normalized to 1) for each iteration for all agents.\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    action_dist = {}\n",
    "    for agent in [\"all\"] + env.agents:\n",
    "        action_dist[agent] = {}\n",
    "        action_dist[agent][\"local\"] = np.empty(iters_n)\n",
    "        action_dist[agent][\"forward\"] = np.empty(iters_n)\n",
    "        action_dist[agent][\"reject\"] = np.empty(iters_n)\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        assert (\n",
    "            raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"] == 1\n",
    "        ), \"Only iterations with one episode are supported!\"\n",
    "\n",
    "        action_dist_tmp = {}\n",
    "        action_dist_tmp[\"local\"] = []\n",
    "        action_dist_tmp[\"forward\"] = []\n",
    "        action_dist_tmp[\"reject\"] = []\n",
    "\n",
    "        for agent in env.agents:\n",
    "            if agent == \"all\":\n",
    "                continue\n",
    "\n",
    "            action_dist_steps = get_normalized_action_step(raw_exp_data, iter_idx, env, agent)\n",
    "\n",
    "            action_dist[agent][\"local\"][iter_idx] = np.average(action_dist_steps[\"local\"])\n",
    "            action_dist[agent][\"forward\"][iter_idx] = np.average(action_dist_steps[\"forward\"])\n",
    "            action_dist[agent][\"reject\"][iter_idx] = np.average(action_dist_steps[\"reject\"])\n",
    "\n",
    "            action_dist_tmp[\"local\"].append(action_dist[agent][\"local\"][iter_idx])\n",
    "            action_dist_tmp[\"forward\"].append(action_dist[agent][\"forward\"][iter_idx])\n",
    "            action_dist_tmp[\"reject\"].append(action_dist[agent][\"reject\"][iter_idx])\n",
    "\n",
    "        action_dist[\"all\"][\"local\"][iter_idx] = np.average(action_dist_tmp[\"local\"])\n",
    "        action_dist[\"all\"][\"forward\"][iter_idx] = np.average(action_dist_tmp[\"forward\"])\n",
    "        action_dist[\"all\"][\"reject\"][iter_idx] = np.average(action_dist_tmp[\"reject\"])\n",
    "\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "action_dist = get_action_distribution(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_action_distribution_plot(action_dist):\n",
    "    for agent, dist in action_dist.items():\n",
    "        plt.close(fig=f\"action_dist_{agent}\")\n",
    "        fig = plt.figure(num=f\"action_dist_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        steps = np.arange(len(raw_exp_data))\n",
    "\n",
    "        ax.stackplot(\n",
    "            steps, dist[\"local\"], dist[\"forward\"], dist[\"reject\"], labels=[\"Local\", \"Forward\", \"Reject\"], alpha=0.8\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Average action distribution per step per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Action proportion\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(25))  # Show x-axis ticks every 50 iterations.\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_action_distribution_plot(action_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Rejections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Differences between reject rates:\n",
    "\n",
    "* **Node reject rate**: it considers the reject rate of the incoming local rate and the _incoming forwarded rate from the neighbors_.\n",
    "* **Agent reject rate**: it is the sum of reject rate decide by the agent (`action_reject`), the rejected incoming local rate (`incoming_rate_reject`) and the rejected forwarded rate (`forward_reject_rate`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Node reject rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reject_data(raw_exp_data, env):\n",
    "    \"\"\"Returns the average node reject rate for each step for each iteration for all agents (including \"all\" agents, which is the sum of all agents).\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reject_data = {}\n",
    "    reject_data[\"all\"] = np.empty(iters_n)\n",
    "    for agent in env.agents:\n",
    "        reject_data[agent] = np.empty(iters_n)\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        assert (\n",
    "            raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"] == 1\n",
    "        ), \"Only iterations with one episode are supported!\"\n",
    "\n",
    "        average_reject_tmp = []\n",
    "        for agent in env.agents:\n",
    "\n",
    "            percent_reject = np.zeros(env.max_steps)\n",
    "            for step in range(env.max_steps):\n",
    "                incoming_rate = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"incoming_rate\"][0][agent][step]\n",
    "                incoming_rate_reject = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"incoming_rate_reject\"][0][\n",
    "                    agent\n",
    "                ][step]\n",
    "\n",
    "                if incoming_rate == 0:\n",
    "                    continue\n",
    "\n",
    "                percent_reject[step] = incoming_rate_reject / incoming_rate * 100\n",
    "\n",
    "            reject_data[agent][iter_idx] = np.average(percent_reject)\n",
    "            average_reject_tmp.append(reject_data[agent][iter_idx])\n",
    "\n",
    "        reject_data[\"all\"][iter_idx] = np.sum(average_reject_tmp)\n",
    "\n",
    "    return reject_data\n",
    "\n",
    "\n",
    "reject_data = get_reject_data(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_average_reject_step_plot(reject_data):\n",
    "    for agent, reject in reject_data.items():\n",
    "        plt.close(fig=f\"average_reject_step_{agent}\")\n",
    "        fig = plt.figure(num=f\"average_reject_step_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        smoothed = savgol_filter(reject, 15, 3)  # window size 15, polynomial order 3\n",
    "\n",
    "        ax.plot(reject, label=\"Original\")\n",
    "        ax.plot(smoothed, label=\"Smoothed\")\n",
    "\n",
    "        ax.set_title(f\"Node reject rate ({agent = })\\n(average node reject rate¹ per step per episode)\")\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.2,\n",
    "            \"¹reject rate = local reject rate + incoming forwarded reject rate\",\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        ax.set_ylabel(\"Reject rate\")\n",
    "        ax.yaxis.set_major_formatter(ticker.PercentFormatter())\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(25))  # Show x-axis ticks every 50 iterations.\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_average_reject_step_plot(reject_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Agent reject rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_reject_rate(raw_exp_data, env):\n",
    "    \"\"\"Returns the average agent reject rate for each step for each iteration for all agents (including \"all\" agents, which is the sum of all agents).\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reject_data = {}\n",
    "    reject_data[\"all\"] = np.empty(iters_n)\n",
    "    for agent in env.agents:\n",
    "        reject_data[agent] = np.empty(iters_n)\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        assert (\n",
    "            raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"] == 1\n",
    "        ), \"Only iterations with one episode are supported!\"\n",
    "\n",
    "        average_reject_tmp = []\n",
    "        for agent in env.agents:\n",
    "\n",
    "            percent_reject = np.zeros(env.max_steps)\n",
    "            for step in range(env.max_steps):\n",
    "                input_rate = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"observation_input_requests\"][0][\n",
    "                    agent\n",
    "                ][step]\n",
    "\n",
    "                action_reject = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"action_reject\"][0][agent][step]\n",
    "                local_reject = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"incoming_rate_local_reject\"][0][\n",
    "                    agent\n",
    "                ][step]\n",
    "                forward_reject = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"forward_reject_rate\"][0][agent][\n",
    "                    step\n",
    "                ]\n",
    "                reject_rate = action_reject + local_reject + forward_reject\n",
    "\n",
    "                if input_rate == 0:\n",
    "                    continue\n",
    "\n",
    "                percent_reject[step] = reject_rate / input_rate * 100\n",
    "\n",
    "            reject_data[agent][iter_idx] = np.average(percent_reject)\n",
    "            average_reject_tmp.append(reject_data[agent][iter_idx])\n",
    "\n",
    "        reject_data[\"all\"][iter_idx] = np.sum(average_reject_tmp)\n",
    "\n",
    "    return reject_data\n",
    "\n",
    "\n",
    "agent_reject_rate = get_agent_reject_rate(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_average_agent_reject_step_plot(reject_data):\n",
    "    for agent, reject in reject_data.items():\n",
    "        plt.close(fig=f\"average_agent_reject_step_{agent}\")\n",
    "        fig = plt.figure(num=f\"average_agent_reject_step_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        smoothed = savgol_filter(reject, 15, 3)  # window size 15, polynomial order 3\n",
    "\n",
    "        ax.plot(reject, label=\"Original\")\n",
    "        ax.plot(smoothed, label=\"Smoothed\")\n",
    "\n",
    "        ax.set_title(f\"Agent reject rate ({agent = })\\n(average agent reject rate¹ per step per episode)\")\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.2,\n",
    "            \"¹reject rate = action reject + local reject + forward reject\",\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        ax.set_ylabel(\"Reject rate\")\n",
    "        ax.yaxis.set_major_formatter(ticker.PercentFormatter())\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(25))  # Show x-axis ticks every 50 iterations.\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_average_agent_reject_step_plot(agent_reject_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Rejections by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reject_rate_dist_by_type(raw_exp_data, env):\n",
    "    \"\"\"Returns the reject rate distribution by type as a ratio to the total reject rate for each step, for each iteration, and for all agents (where \"all\" agent is just the sum of all agent values). Each value is the average for one iteration (= one episode).\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reject_types = [\"excess_local\", \"action\", \"excess_forward\"]\n",
    "\n",
    "    # Prepare the output dictionary.\n",
    "    reject_data = {\n",
    "        agent: {reject_type: np.zeros(iters_n) for reject_type in reject_types} for agent in [\"all\"] + env.agents\n",
    "    }\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        assert (\n",
    "            raw_exp_data[iter_idx][\"env_runners\"][\"episodes_this_iter\"] == 1\n",
    "        ), \"Only iterations with one episode are supported!\"\n",
    "\n",
    "        action_reject_tmp, local_reject_tmp, forward_reject_tmp = [], [], []\n",
    "        for agent in env.agents:\n",
    "            # Reject rate arrays (each index is one step).\n",
    "            action_reject = np.array(raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"action_reject\"][0][agent])\n",
    "            local_reject = np.array(\n",
    "                raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"incoming_rate_local_reject\"][0][agent]\n",
    "            )\n",
    "            forward_reject = np.array(\n",
    "                raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"forward_reject_rate\"][0][agent]\n",
    "            )\n",
    "            total_reject = action_reject + local_reject + forward_reject\n",
    "\n",
    "            assert total_reject.sum() > 0\n",
    "            reject_data[agent][\"action\"][iter_idx] = action_reject.sum() / total_reject.sum()\n",
    "            reject_data[agent][\"excess_local\"][iter_idx] = local_reject.sum() / total_reject.sum()\n",
    "            reject_data[agent][\"excess_forward\"][iter_idx] = forward_reject.sum() / total_reject.sum()\n",
    "\n",
    "            action_reject_tmp.append(reject_data[agent][\"action\"][iter_idx])\n",
    "            local_reject_tmp.append(reject_data[agent][\"excess_local\"][iter_idx])\n",
    "            forward_reject_tmp.append(reject_data[agent][\"excess_forward\"][iter_idx])\n",
    "\n",
    "        # Calculate the average distribution for all agents.\n",
    "        action_reject = np.array(action_reject_tmp)\n",
    "        local_reject = np.array(local_reject_tmp)\n",
    "        forward_reject = np.array(forward_reject_tmp)\n",
    "        total_reject = action_reject + local_reject + forward_reject\n",
    "        assert total_reject.sum() > 0\n",
    "        reject_data[\"all\"][\"action\"][iter_idx] = action_reject.sum() / total_reject.sum()\n",
    "        reject_data[\"all\"][\"excess_local\"][iter_idx] = local_reject.sum() / total_reject.sum()\n",
    "        reject_data[\"all\"][\"excess_forward\"][iter_idx] = forward_reject.sum() / total_reject.sum()\n",
    "\n",
    "    return reject_data\n",
    "\n",
    "\n",
    "reject_rate_dist_by_type = get_reject_rate_dist_by_type(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reject_type_distribution_plot(reject_rate_dist_by_type):\n",
    "    for agent, dist in reject_rate_dist_by_type.items():\n",
    "        plt.close(fig=f\"reject_type_distribution_{agent}\")\n",
    "        fig = plt.figure(num=f\"reject_type_distribution_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        iter_n = np.arange(len(raw_exp_data))\n",
    "\n",
    "        ax.stackplot(\n",
    "            iter_n,\n",
    "            dist[\"action\"],\n",
    "            dist[\"excess_local\"],\n",
    "            dist[\"excess_forward\"],\n",
    "            labels=[\"By action\", \"Excess local\", \"Excess forward\"],\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Average reject type per step per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Reject distribution\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(25))  # Show x-axis ticks every 50 iterations.\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_reject_type_distribution_plot(reject_rate_dist_by_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
