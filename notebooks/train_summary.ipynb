{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Train Summary Plots/Anaylisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Experiment selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports.\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib widget\n",
    "import base\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import ipywidgets\n",
    "\n",
    "import dfaas_env\n",
    "import dfaas_utils\n",
    "import dfaas_upperbound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Select one or more experiments to view.\n",
    "\n",
    "**WARNING**: If multiple experiments are selected, they must share the same number of training iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = base.get_experiments(\"/home/emanuele/marl-dfaas/results\")\n",
    "\n",
    "# Show the name as the portion of the path after \"results\",\n",
    "# but anyway the values are full Path objects.\n",
    "exp_select = ipywidgets.SelectMultiple(\n",
    "    options=experiments,\n",
    "    index=[0],\n",
    "    description=\"Experiment(s):\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=ipywidgets.Layout(width=\"70%\"),\n",
    ")\n",
    "\n",
    "ipywidgets.AppLayout(center=exp_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Experiment loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "This section must be run before any of the following sections to load the selected experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_dir = exp_select.value\n",
    "assert len(exps_dir) > 0, \"must select at least one experiment\"\n",
    "\n",
    "# Preload the data (result.json file) for all selected experiments.\n",
    "raw_exp_data = {}\n",
    "for exp_dir in exps_dir:\n",
    "    raw_exp_data[exp_dir] = dfaas_utils.parse_result_file(exp_dir / \"result.json\")\n",
    "\n",
    "# Create the reference environment based on DFaaS.\n",
    "for exp_dir in exps_dir:\n",
    "    env = base.get_env(exp_dir)\n",
    "    if env.__class__ == dfaas_env.DFaaS:\n",
    "        break\n",
    "\n",
    "# At least one experiment must be of type DFaaS. SingleDFaaS is used only as\n",
    "# reference.\n",
    "assert env.__class__ == dfaas_env.DFaaS, f\"{env.__class__}\"\n",
    "\n",
    "print(\"Selected experiments:\")\n",
    "for exp_dir in exps_dir:\n",
    "    print(f\"  - {exp_dir.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common functions for average reward data.\n",
    "\n",
    "\n",
    "def average_reward_step(iter, agent):\n",
    "    \"\"\"Returns the average reward per step for the given iteration and agent.\"\"\"\n",
    "    episodes = iter[\"env_runners\"][\"episodes_this_iter\"]\n",
    "\n",
    "    tmp = np.empty(episodes, dtype=np.float32)\n",
    "    for epi_idx in range(episodes):\n",
    "        tmp[epi_idx] = np.average(\n",
    "            iter[\"env_runners\"][\"hist_stats\"][\"reward\"][epi_idx][agent]\n",
    "        )\n",
    "\n",
    "    return np.average(tmp)\n",
    "\n",
    "\n",
    "def get_reward_data(iter_data):\n",
    "    final_data = {}\n",
    "\n",
    "    for exp_dir, iters in iter_data.items():\n",
    "        env = base.get_env(exp_dir)\n",
    "        if env.__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "            # The upperbound is not used for the reward plots.\n",
    "            continue\n",
    "\n",
    "        data = {}\n",
    "        agents = env.agents\n",
    "\n",
    "        data[\"agents\"] = agents\n",
    "        data[\"iterations\"] = len(iters)\n",
    "        data[\"episodes\"] = iters[0][\"env_runners\"][\"episodes_this_iter\"]\n",
    "\n",
    "        reward_total_avg = {}  # Average total reward per episode.\n",
    "        reward_step_avg = {}  # Average reward per step.\n",
    "\n",
    "        reward_total_avg[\"all\"] = np.empty(data[\"iterations\"], dtype=np.float32)\n",
    "        for agent in data[\"agents\"]:\n",
    "            reward_total_avg[agent] = np.empty(data[\"iterations\"], dtype=np.float32)\n",
    "            reward_step_avg[agent] = np.empty(data[\"iterations\"], dtype=np.float32)\n",
    "\n",
    "        # For each iteration, get the average reward, since there are multiple\n",
    "        # episodes played in each iteration.\n",
    "        for iter in iters:\n",
    "            # Index starts from one in log files, but Python list from zero.\n",
    "            iter_idx = iter[\"training_iteration\"] - 1\n",
    "\n",
    "            reward_total_avg[\"all\"][iter_idx] = np.average(\n",
    "                iter[\"env_runners\"][\"hist_stats\"][\"episode_reward\"]\n",
    "            )\n",
    "\n",
    "            for agent in data[\"agents\"]:\n",
    "                reward_total_avg[agent][iter_idx] = np.average(\n",
    "                    iter[\"env_runners\"][\"hist_stats\"][f\"policy_policy_{agent}_reward\"]\n",
    "                )\n",
    "                reward_step_avg[agent][iter_idx] = average_reward_step(iter, agent)\n",
    "\n",
    "        data[\"reward_total_avg\"] = reward_total_avg\n",
    "        data[\"reward_step_avg\"] = reward_step_avg\n",
    "\n",
    "        final_data[exp_dir] = data\n",
    "\n",
    "    return final_data\n",
    "\n",
    "\n",
    "reward_data = get_reward_data(raw_exp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Average per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### All agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reward_plot_all_agents():\n",
    "    plt.close(fig=\"reward_all\")\n",
    "    fig = plt.figure(num=\"reward_all\", layout=\"constrained\")\n",
    "    fig.canvas.header_visible = False\n",
    "    ax = fig.subplots()\n",
    "\n",
    "    ax.axhline(\n",
    "        y=env.max_steps * len(env.agents), color=\"red\", linestyle=\"--\", label=\"Limit\"\n",
    "    )\n",
    "    for exp_dir in exps_dir:\n",
    "        if base.get_env(exp_dir).__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "            # The upperbound is not used for the reward plots.\n",
    "            continue\n",
    "        ax.plot(reward_data[exp_dir][\"reward_total_avg\"][\"all\"], label=exp_dir.name)\n",
    "\n",
    "    ax.set_title(\"Average reward per episode (all agents)\")\n",
    "\n",
    "    ax.set_ylabel(\"Reward per episode\")\n",
    "    ax.set_ylim(bottom=0, top=env.max_steps * len(env.agents) * 1.05)\n",
    "\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.xaxis.set_major_locator(\n",
    "        ticker.MultipleLocator(50)\n",
    "    )  # Show x-axis ticks every 50 iterations.\n",
    "\n",
    "    ax.legend(loc=\"lower center\")\n",
    "    ax.grid(axis=\"both\")\n",
    "    ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_reward_plot_all_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Single agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_reward_plot_single_agents():\n",
    "    for agent in env.agents:\n",
    "        plt.close(fig=f\"reward_{agent}\")\n",
    "        fig = plt.figure(num=f\"reward_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.axhline(y=env.max_steps, color=\"red\", linestyle=\"--\", label=\"Limit\")\n",
    "        for exp_dir in exps_dir:\n",
    "            if base.get_env(exp_dir).__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "                # The upperbound is not used for the reward plots.\n",
    "                continue\n",
    "            ax.plot(reward_data[exp_dir][\"reward_total_avg\"][agent], label=exp_dir.name)\n",
    "\n",
    "        ax.set_title(f\"Average reward per episode ({agent = })\")\n",
    "\n",
    "        ax.set_ylabel(\"Reward per episode\")\n",
    "        ax.set_ylim(bottom=0, top=env.max_steps * 1.05)\n",
    "\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.xaxis.set_major_locator(\n",
    "            ticker.MultipleLocator(50)\n",
    "        )  # Show x-axis ticks every 50 iterations.\n",
    "\n",
    "        ax.legend(loc=\"lower center\")\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_reward_plot_single_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Processed requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common functions for processed requests.\n",
    "\n",
    "\n",
    "def get_processed_requests_data_upperbound(iters):\n",
    "    iterations = len(iters)\n",
    "    data = {\"input_reqs\": np.empty(iterations), \"processed_reqs\": np.empty(iterations)}\n",
    "\n",
    "    # Scan each iteration.\n",
    "    for iter_idx in range(iterations):\n",
    "        iter_data = iters[iter_idx][\"env_runners\"]\n",
    "\n",
    "        episodes = iter_data[\"episodes_this_iter\"]\n",
    "        input_reqs_iter = np.empty(episodes)\n",
    "        processed_local_iter = np.empty(episodes)\n",
    "\n",
    "        # Iterate the episodes.\n",
    "        for epi_idx in range(episodes):\n",
    "            input_reqs = np.sum(\n",
    "                iter_data[\"hist_stats\"][\"observation_input_requests\"][epi_idx]\n",
    "            )\n",
    "            processed_local = np.sum(\n",
    "                iter_data[\"hist_stats\"][\"processed_local\"][epi_idx]\n",
    "            )\n",
    "\n",
    "            input_reqs_iter[epi_idx] = input_reqs\n",
    "            processed_local_iter[epi_idx] = processed_local\n",
    "\n",
    "        # Update iteration data (average the episodes values).\n",
    "        data[\"input_reqs\"][iter_idx] = np.average(input_reqs_iter)\n",
    "        data[\"processed_reqs\"][iter_idx] = np.average(processed_local_iter)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_processed_requests_data_episode(iter_data, epi_idx, env):\n",
    "    iter_data = iter_data[\"env_runners\"][\"hist_stats\"]\n",
    "\n",
    "    data = defaultdict(lambda: defaultdict())\n",
    "    (\n",
    "        data[\"all\"][\"processed_reqs\"],\n",
    "        data[\"all\"][\"input_reqs\"],\n",
    "        data[\"all\"][\"processed_forwarded_reqs\"],\n",
    "    ) = (0, 0, 0)\n",
    "    for agent in env.agents:\n",
    "        processed_reqs = np.sum(iter_data[\"processed_local\"][epi_idx][agent])\n",
    "        input_reqs = np.sum(iter_data[\"observation_input_requests\"][epi_idx][agent])\n",
    "\n",
    "        try:\n",
    "            processed_forward = np.sum(\n",
    "                iter_data[\"processed_local_forward\"][epi_idx][agent]\n",
    "            )\n",
    "        except KeyError:\n",
    "            # May be missing if the agent did not receive any forwarded request.\n",
    "            processed_forward = 0\n",
    "\n",
    "        data[agent][\"processed_reqs\"] = processed_reqs\n",
    "        data[agent][\"input_reqs\"] = input_reqs\n",
    "        data[agent][\"processed_forwarded_reqs\"] = processed_forward\n",
    "        data[\"all\"][\"processed_reqs\"] += processed_reqs\n",
    "        data[\"all\"][\"input_reqs\"] += input_reqs\n",
    "        data[\"all\"][\"processed_forwarded_reqs\"] += processed_forward\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_processed_requests_data(iter_data):\n",
    "    #  is a disctionary with three levels of depth (experiment, iteration, metrics).\n",
    "    final_data = defaultdict(lambda: defaultdict(lambda: defaultdict()))\n",
    "\n",
    "    for exp_dir, iters in iter_data.items():\n",
    "        env = base.get_env(exp_dir)\n",
    "        if env.__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "            # The upperbound data extraction is different from the normal flow.\n",
    "            final_data[exp_dir] = get_processed_requests_data_upperbound(iters)\n",
    "            continue\n",
    "\n",
    "        agents = env.agents\n",
    "        iterations = len(iters)\n",
    "\n",
    "        # Create the portion of the dictionary for this experiment that\n",
    "        # contains the average values of the metrics for each iteration.\n",
    "        for agent in [\"all\"] + env.agents:\n",
    "            for key in [\"input_reqs\", \"processed_reqs\", \"processed_forwarded_reqs\"]:\n",
    "                final_data[exp_dir][agent][key] = np.empty(iterations)\n",
    "\n",
    "        # For each iteration, calculate the metrics for each episode played,\n",
    "        # then average the values for the number of episodes of that iteration.\n",
    "        for iter_idx in range(iterations):\n",
    "            episodes = iters[iter_idx][\"env_runners\"][\"episodes_this_iter\"]\n",
    "\n",
    "            # Create the data dictionary that contains the metrics for each\n",
    "            # episode in this iteration.\n",
    "            data = defaultdict(lambda: defaultdict())\n",
    "            for agent in [\"all\"] + env.agents:\n",
    "                for key in [\"input_reqs\", \"processed_reqs\", \"processed_forwarded_reqs\"]:\n",
    "                    data[agent][key] = np.empty(episodes, dtype=np.int32)\n",
    "\n",
    "            # Iterate the episodes.\n",
    "            for epi_idx in range(episodes):\n",
    "                data_epi = get_processed_requests_data_episode(\n",
    "                    iters[iter_idx], epi_idx, env\n",
    "                )\n",
    "\n",
    "                for agent in [\"all\"] + env.agents:\n",
    "                    for key in [\n",
    "                        \"input_reqs\",\n",
    "                        \"processed_reqs\",\n",
    "                        \"processed_forwarded_reqs\",\n",
    "                    ]:\n",
    "                        data[agent][key][epi_idx] = data_epi[agent][key]\n",
    "\n",
    "            # Update iteration data.\n",
    "            for agent in [\"all\"] + env.agents:\n",
    "                for key in [\"input_reqs\", \"processed_reqs\", \"processed_forwarded_reqs\"]:\n",
    "                    final_data[exp_dir][agent][key][iter_idx] = np.average(\n",
    "                        data[agent][key]\n",
    "                    )\n",
    "\n",
    "    return final_data\n",
    "\n",
    "\n",
    "processed_reqs_data = get_processed_requests_data(raw_exp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Average processed requests per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### All agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_avg_processed_reqs_plot_all_agents():\n",
    "    plt.close(fig=\"avg_processed_reqs_all_agents\")\n",
    "    fig = plt.figure(num=\"avg_processed_reqs_all_agents\", layout=\"constrained\")\n",
    "    fig.canvas.header_visible = False\n",
    "    ax = fig.subplots()\n",
    "\n",
    "    data = processed_reqs_data  # Alias for better readability.\n",
    "    for exp_dir in exps_dir:\n",
    "        if base.get_env(exp_dir).__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "            # This is the upperbound case.\n",
    "            ratios = data[exp_dir][\"processed_reqs\"] / data[exp_dir][\"input_reqs\"]\n",
    "            ax.plot(ratios, label=\"Upperbound\")\n",
    "            continue\n",
    "\n",
    "        ratios = (\n",
    "            data[exp_dir][\"all\"][\"processed_reqs\"] / data[exp_dir][\"all\"][\"input_reqs\"]\n",
    "        )\n",
    "        ax.plot(ratios, label=exp_dir.name)\n",
    "\n",
    "    ax.set_title(\"Average processed requests per episode (all agents)\")\n",
    "\n",
    "    ax.set_ylabel(\"Requests\")\n",
    "    ax.yaxis.set_major_formatter(ticker.PercentFormatter(1.0))\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "    ax.legend(loc=\"lower center\")\n",
    "    ax.grid(axis=\"both\")\n",
    "    ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_avg_processed_reqs_plot_all_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Single agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_avg_processed_reqs_plot_single_agents():\n",
    "    for agent in env.agents:\n",
    "        plt.close(fig=f\"avg_processed_reqs_{agent}\")\n",
    "        fig = plt.figure(num=f\"avg_processed_reqs_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        data = processed_reqs_data  # Alias for better readability.\n",
    "        for exp_dir in exps_dir:\n",
    "            if base.get_env(exp_dir).__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "                # The upperbound is valid only for global view (all agents).\n",
    "                continue\n",
    "\n",
    "            ratios = (\n",
    "                data[exp_dir][agent][\"processed_reqs\"]\n",
    "                / data[exp_dir][agent][\"input_reqs\"]\n",
    "            )\n",
    "            ax.plot(ratios, label=exp_dir.name)\n",
    "\n",
    "        ax.set_title(f\"Average processed requests per episode ({agent = })\")\n",
    "\n",
    "        ax.set_ylabel(\"Requests\")\n",
    "        ax.yaxis.set_major_formatter(ticker.PercentFormatter(1.0))\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_avg_processed_reqs_plot_single_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Average processed forwarded requests per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### All agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_avg_processed_fw_reqs_plot_all_agents():\n",
    "    plt.close(fig=\"avg_processed_fw_reqs\")\n",
    "    fig = plt.figure(num=\"avg_processed_fw_reqs\", layout=\"constrained\")\n",
    "    fig.canvas.header_visible = False\n",
    "    ax = fig.subplots()\n",
    "\n",
    "    data = processed_reqs_data  # Alias for better readability.\n",
    "    for exp_dir in exps_dir:\n",
    "        if base.get_env(exp_dir).__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "            # The upperbound is valid only for total processed requests.\n",
    "            continue\n",
    "\n",
    "        ratios_forwarded = (\n",
    "            data[exp_dir][\"all\"][\"processed_forwarded_reqs\"]\n",
    "            / data[exp_dir][\"all\"][\"processed_reqs\"]\n",
    "        )\n",
    "        ax.plot(ratios_forwarded, label=exp_dir.name)\n",
    "\n",
    "    ax.set_title(\"Average processed forwarded requests per episode (all agents)\")\n",
    "\n",
    "    ax.set_ylabel(\"Requests\")\n",
    "    ax.yaxis.set_major_formatter(ticker.PercentFormatter(1.0))\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"both\")\n",
    "    ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_avg_processed_fw_reqs_plot_all_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Single agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_avg_processed_fw_reqs_plot_single_agents():\n",
    "    for agent in env.agents:\n",
    "        plt.close(fig=f\"avg_processed_fw_reqs_{agent}\")\n",
    "        fig = plt.figure(num=f\"avg_processed_fw_reqs_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        data = processed_reqs_data  # Alias for better readability.\n",
    "        for exp_dir in exps_dir:\n",
    "            if base.get_env(exp_dir).__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "                # The upperbound is valid only for total processed requests.\n",
    "                continue\n",
    "\n",
    "            ratios_forwarded = (\n",
    "                data[exp_dir][agent][\"processed_forwarded_reqs\"]\n",
    "                / data[exp_dir][agent][\"processed_reqs\"]\n",
    "            )\n",
    "            ax.plot(ratios_forwarded, label=exp_dir.name)\n",
    "\n",
    "        ax.set_title(f\"Average processed forwarded requests per episode ({agent = })\")\n",
    "\n",
    "        ax.set_ylabel(\"Requests\")\n",
    "        ax.yaxis.set_major_formatter(ticker.PercentFormatter(1.0))\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_avg_processed_fw_reqs_plot_single_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "To show some plots about the queue of agents, we first need to collect the data. Note that since this notebook is for summarised training for all iterations, the plots are an average of all episodes in an iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common functions for queue.\n",
    "\n",
    "\n",
    "def get_queue_data_upperbound(env, iters):\n",
    "    iterations = len(iters)\n",
    "    data = {\n",
    "        \"queue_size\": np.empty(iterations),\n",
    "        \"queue_full\": np.empty(iterations),\n",
    "        \"queue_size_std\": np.empty(iterations),\n",
    "    }\n",
    "\n",
    "    # Scan each iteration.\n",
    "    for iter_idx in range(iterations):\n",
    "        iter_data = iters[iter_idx][\"env_runners\"]\n",
    "\n",
    "        episodes = iter_data[\"episodes_this_iter\"]\n",
    "\n",
    "        queue_size = np.empty(episodes)\n",
    "        queue_size_std = np.empty(episodes)\n",
    "        queue_full = np.empty(episodes)\n",
    "\n",
    "        # Iterate the episodes.\n",
    "        for epi_idx in range(episodes):\n",
    "            queue = np.array(iter_data[\"hist_stats\"][\"queue_size\"][epi_idx])\n",
    "\n",
    "            queue_size[epi_idx] = np.mean(queue)\n",
    "            queue_size_std[epi_idx] = np.std(queue)\n",
    "            queue_full[epi_idx] = np.where(queue == env.queue_capacity)[0].size\n",
    "\n",
    "        # Update iteration data (average the episodes values).\n",
    "        data[\"queue_size\"][iter_idx] = np.average(queue_size)\n",
    "        data[\"queue_size_std\"][iter_idx] = np.average(queue_size_std)\n",
    "        data[\"queue_full\"][iter_idx] = np.average(queue_full)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_queue_data(iter_data):\n",
    "    #  is a disctionary with three levels of depth (experiment, iteration, metrics).\n",
    "    final_data = defaultdict(lambda: defaultdict(lambda: defaultdict()))\n",
    "\n",
    "    for exp_dir, iters in iter_data.items():\n",
    "        env = base.get_env(exp_dir)\n",
    "        if env.__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "            # The upperbound data extraction is different from the normal flow.\n",
    "            final_data[exp_dir] = get_queue_data_upperbound(env, iters)\n",
    "            continue\n",
    "\n",
    "        iterations = len(iters)\n",
    "\n",
    "        # Keys of the dictionary for each agent.\n",
    "        metrics = [\"queue_size_mean\", \"queue_size_std\", \"queue_full\"]\n",
    "\n",
    "        # Create the portion of the dictionary for this experiment that\n",
    "        # contains the average values of the metrics for each iteration.\n",
    "        for agent in [\"all\"] + env.agents:\n",
    "            for metric in metrics:\n",
    "                final_data[exp_dir][agent][metric] = np.empty(iterations)\n",
    "\n",
    "        # For each iteration, calculate the metrics for each episode played,\n",
    "        # then average the values for the number of episodes of that iteration.\n",
    "        for iter_idx in range(iterations):\n",
    "            episodes = iters[iter_idx][\"env_runners\"][\"episodes_this_iter\"]\n",
    "            hist_stats = iters[iter_idx][\"env_runners\"][\"hist_stats\"]\n",
    "\n",
    "            # Temporary dictionary for each iteration.\n",
    "            epi_data = defaultdict(lambda: defaultdict())\n",
    "            for agent in [\"all\"] + env.agents:\n",
    "                for metric in metrics:\n",
    "                    epi_data[agent][metric] = np.empty(episodes)\n",
    "\n",
    "            # Iterate the episodes.\n",
    "            for epi_idx in range(episodes):\n",
    "                queue_all = np.empty(0)\n",
    "\n",
    "                for agent in env.agents:\n",
    "                    queue = np.array(hist_stats[\"queue_size\"][epi_idx][agent])\n",
    "                    queue_all = np.concatenate([queue_all, queue])\n",
    "\n",
    "                    epi_data[agent][\"queue_size_mean\"][epi_idx] = np.mean(queue)\n",
    "                    epi_data[agent][\"queue_size_std\"][epi_idx] = np.std(queue)\n",
    "\n",
    "                    # Count the number of occurencies where queue is full\n",
    "                    epi_data[agent][\"queue_full\"][epi_idx] = np.where(\n",
    "                        queue == env.queue_capacity\n",
    "                    )[0].size\n",
    "\n",
    "                epi_data[\"all\"][\"queue_size_mean\"][epi_idx] = np.mean(queue_all)\n",
    "                epi_data[\"all\"][\"queue_size_std\"][epi_idx] = np.std(queue_all)\n",
    "                epi_data[\"all\"][\"queue_full\"][epi_idx] = np.where(\n",
    "                    queue == env.queue_capacity\n",
    "                )[0].size\n",
    "\n",
    "            # Update iteration data.\n",
    "            for agent in [\"all\"] + env.agents:\n",
    "                for metric in metrics:\n",
    "                    mean = np.mean(epi_data[agent][metric])\n",
    "                    final_data[exp_dir][agent][metric][iter_idx] = mean\n",
    "\n",
    "    return final_data\n",
    "\n",
    "\n",
    "queue_data = get_queue_data(raw_exp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Average queue size per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### All agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_avg_queue_size_plot_all_agents():\n",
    "    plt.close(fig=\"avg_queue_size\")\n",
    "    fig = plt.figure(num=\"avg_queue_size\", layout=\"constrained\")\n",
    "    fig.canvas.header_visible = False\n",
    "    ax = fig.subplots()\n",
    "\n",
    "    for exp_dir in exps_dir:\n",
    "        if base.get_env(exp_dir).__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "            # This is the upperbound case.\n",
    "            ax.plot(\n",
    "                queue_data[exp_dir][\"queue_size\"] / env.queue_capacity,\n",
    "                label=\"Upperbound\",\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        ax.plot(\n",
    "            queue_data[exp_dir][\"all\"][\"queue_size_mean\"] / env.queue_capacity,\n",
    "            label=exp_dir.name,\n",
    "        )\n",
    "\n",
    "    ax.set_title(\"Average queue size per episode (all agents)\")\n",
    "\n",
    "    ax.set_ylabel(\"Requests in queue\")\n",
    "    ax.yaxis.set_major_formatter(ticker.PercentFormatter(1.0))\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"both\")\n",
    "    ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_avg_queue_size_plot_all_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Single agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_avg_queue_size_plot_single_agents():\n",
    "    for agent in env.agents:\n",
    "        plt.close(fig=f\"avg_queue_size_{agent}\")\n",
    "        fig = plt.figure(num=f\"avg_queue_size_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        for exp_dir in exps_dir:\n",
    "            if base.get_env(exp_dir).__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "                # The upperbound is valid only for all agents.\n",
    "                continue\n",
    "\n",
    "            ax.plot(\n",
    "                queue_data[exp_dir][agent][\"queue_size_mean\"] / env.queue_capacity,\n",
    "                label=exp_dir.name,\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"Average queue size per episode ({agent = })\")\n",
    "\n",
    "        ax.set_ylabel(\"Requests in queue\")\n",
    "        ax.yaxis.set_major_formatter(ticker.PercentFormatter(1.0))\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.legend(loc=\"lower center\")\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_avg_queue_size_plot_single_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Average queue full per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### All agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_queue_full_plot_all_agents():\n",
    "    plt.close(fig=\"avg_queue_full\")\n",
    "    fig = plt.figure(num=\"avg_queue_full\", layout=\"constrained\")\n",
    "    fig.canvas.header_visible = False\n",
    "    ax = fig.subplots()\n",
    "\n",
    "    ax.axhline(\n",
    "        y=env.max_steps * len(env.agents),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Upper Limit\",\n",
    "    )\n",
    "    for exp_dir in exps_dir:\n",
    "        if base.get_env(exp_dir).__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "            # This is the upperbound case.\n",
    "            ax.plot(queue_data[exp_dir][\"queue_full\"], label=\"Upperbound\")\n",
    "            continue\n",
    "\n",
    "        ax.plot(queue_data[exp_dir][\"all\"][\"queue_full\"], label=exp_dir.name)\n",
    "\n",
    "    ax.set_title(\"Times the queue is full per episode (all agents)\")\n",
    "\n",
    "    ax.set_ylabel(\"Queue full\")\n",
    "    ax.set_ylim(bottom=0, top=env.max_steps * len(env.agents) * 1.05)\n",
    "\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"both\")\n",
    "    ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_queue_full_plot_all_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### Single agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_queue_full_plot_single_agents():\n",
    "    for agent in env.agents:\n",
    "        plt.close(fig=f\"avg_queue_full_{agent}\")\n",
    "        fig = plt.figure(num=f\"avg_queue_full_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.axhline(\n",
    "            y=env.max_steps,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            label=\"Upper Limit\",\n",
    "        )\n",
    "        for exp_dir in exps_dir:\n",
    "            if base.get_env(exp_dir).__class__ == dfaas_upperbound.SingleDFaaS:\n",
    "                # The upperbound is valid only for all agents.\n",
    "                continue\n",
    "\n",
    "            ax.plot(queue_data[exp_dir][agent][\"queue_full\"], label=exp_dir.name)\n",
    "\n",
    "        ax.set_title(f\"Times the queue is full per episode ({agent = })\")\n",
    "\n",
    "        ax.set_ylabel(\"Queue full\")\n",
    "        ax.set_ylim(bottom=0, top=env.max_steps * 1.05)\n",
    "\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_queue_full_plot_single_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Replay Buffer (only for SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "This section shows some metrics related to the replay buffer. It is specific to SAC experiments only, so other experiments will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.sac import SAC\n",
    "\n",
    "\n",
    "def filter_sac(raw_exp_dir):\n",
    "    \"\"\"Returns a copy of raw_exp_dir with only experiments trained with the SAC\n",
    "    algorithm. It may be empty!\"\"\"\n",
    "    sac_raw_exp_dir = {}\n",
    "    for exp_dir in raw_exp_data:\n",
    "        config = base.get_exp_config(exp_dir)\n",
    "        if config.algo_class is SAC:\n",
    "            sac_raw_exp_dir[exp_dir] = raw_exp_data[exp_dir]\n",
    "\n",
    "    return sac_raw_exp_dir\n",
    "\n",
    "\n",
    "sac_raw_exp_dir = filter_sac(raw_exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction for replay buffer.\n",
    "\n",
    "\n",
    "def get_replay_buffer_data(raw_exp_data):\n",
    "    # Returned dict with three levels of depth (experiment, metrics, iterations).\n",
    "    final_data = defaultdict(lambda: defaultdict(lambda: defaultdict()))\n",
    "\n",
    "    for exp_dir, iters in raw_exp_data.items():\n",
    "        iterations = len(iters)\n",
    "        config = base.get_exp_config(exp_dir)\n",
    "        policies = list(config.policies.keys())\n",
    "\n",
    "        final_data[exp_dir][\"policies\"] = policies\n",
    "        final_data[exp_dir][\"capacity_per_policy\"] = iters[0][\"info\"][\"replay_buffer\"][\n",
    "            \"capacity_per_policy\"\n",
    "        ]\n",
    "        for policy in policies:\n",
    "            final_data[exp_dir][\"num_entries\"][policy] = np.empty(\n",
    "                iterations, dtype=np.int64\n",
    "            )\n",
    "            final_data[exp_dir][\"sampled\"][policy] = np.empty(\n",
    "                iterations, dtype=np.int64\n",
    "            )\n",
    "\n",
    "        for iter_idx in range(iterations):\n",
    "            for policy in policies:\n",
    "                num_entries = iters[iter_idx][\"info\"][\"replay_buffer\"][\n",
    "                    f\"policy_{policy}\"\n",
    "                ][\"num_entries\"]\n",
    "                sampled = iters[iter_idx][\"info\"][\"replay_buffer\"][f\"policy_{policy}\"][\n",
    "                    \"sampled_count\"\n",
    "                ]\n",
    "\n",
    "                final_data[exp_dir][\"num_entries\"][policy][iter_idx] = num_entries\n",
    "                final_data[exp_dir][\"sampled\"][policy][iter_idx] = sampled\n",
    "\n",
    "    return final_data\n",
    "\n",
    "\n",
    "if len(sac_raw_exp_dir) > 0:\n",
    "    replay_buffer_data = get_replay_buffer_data(raw_exp_data)\n",
    "else:\n",
    "    print(\"Skipping section since there are not experiments using SAC!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_replay_buffer_plot():\n",
    "    for exp_dir in exps_dir:\n",
    "        plt.close(fig=f\"replay_buffer_{exp_dir.name}\")\n",
    "        fig = plt.figure(num=f\"replay_buffer_{exp_dir.name}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "\n",
    "        ax = fig.subplots()\n",
    "        # Show the plof only for one policy, since all policies have the same\n",
    "        # replay buffer capacity and behaviour.\n",
    "        policy = replay_buffer_data[exp_dir][\"policies\"][0]\n",
    "\n",
    "        ax.plot(\n",
    "            replay_buffer_data[exp_dir][\"num_entries\"][policy], label=\"Stored entries\"\n",
    "        )\n",
    "        ax.plot(replay_buffer_data[exp_dir][\"sampled\"][policy], label=\"Sampled entries\")\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"Replay buffer status per iteration for one policy and\\nexperiment {exp_dir.name!r}\"\n",
    "        )\n",
    "\n",
    "        ax.set_ylabel(\"Entries\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "if len(sac_raw_exp_dir) > 0:\n",
    "    make_replay_buffer_plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
