{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Train summary for a single episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook presents plots related to a single episode. It can display plots about an episode from two sources:\n",
    "\n",
    "1. **Training iterations**: each iteration plays the same number of episodes, but the episodes are different as they use randomly generated seeds.\n",
    "\n",
    "2. **Evaluation iterations**: each iteration also plays the same number of episodes, but they use the same list of seeds across iterations. This is to evaluate the agents over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Experiment loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports.\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib widget\n",
    "import base\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import dfaas_env\n",
    "import dfaas_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory where the experiments are located..\n",
    "prefix_dir = Path(\"/home/emanuele/marl-dfaas/results/\")\n",
    "\n",
    "# Experiment directory.\n",
    "exp_dir = prefix_dir / \"DF_20250707_165017_PPO_test\"\n",
    "\n",
    "# Which training statistics to show: from training iterations or evaluation\n",
    "# iterations.\n",
    "mode = \"eval\"  # Or \"eval\"\n",
    "\n",
    "if mode == \"train\":\n",
    "    # The result file is a JSON with each entry separated by newline.\n",
    "    raw_exp_data = dfaas_utils.parse_result_file(exp_dir / \"result.json\")\n",
    "elif mode == \"eval\":\n",
    "    # The evaluation file is instead a normal JSON.\n",
    "    raw_exp_data = dfaas_utils.json_to_dict(exp_dir / \"evaluation.json\")\n",
    "else:\n",
    "    raise ValueError(f\"Invalid {mode = }\")\n",
    "\n",
    "# Reference environment.\n",
    "env = base.get_env(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Experiment prefix dir: {prefix_dir.as_posix()!r}\")\n",
    "print(f\"Experiment name:       {exp_dir.name!r}\")\n",
    "print(f\"Agents:                {env.agents} ({len(env.agents)})\")\n",
    "print(f\"Mode:                  {mode}\")\n",
    "if mode == \"train\":\n",
    "    print(f\"Iterations:            {len(raw_exp_data)}\")\n",
    "else:\n",
    "    print(f\"Evaluations:           {len(raw_exp_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which iteration (training or evaluation) to select?\n",
    "iteration_idx = 2\n",
    "\n",
    "# Which episode from the single iteration to select?\n",
    "episode_idx = 0\n",
    "\n",
    "assert 0 <= iteration_idx <= len(raw_exp_data) - 1, \"iteration_idx must be a valid iteration index!\"\n",
    "\n",
    "assert (\n",
    "    0 <= episode_idx < raw_exp_data[iteration_idx][\"env_runners\"][\"num_episodes\"]\n",
    "), \"episode_idx must be a valid episode index!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Selected iteration:    {iteration_idx}\")\n",
    "print(f\"Selected episode:      {episode_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reward_plot(raw_exp_data, env, iteration_idx, episode_idx):\n",
    "    for agent in [\"all\"] + env.agents:\n",
    "        plt.close(fig=f\"reward_{agent}\")\n",
    "        fig = plt.figure(num=f\"reward_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        if agent == \"all\":\n",
    "            reward = np.add(\n",
    "                *[\n",
    "                    raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"reward\"][episode_idx][agent]\n",
    "                    for agent in env.agents\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            reward = raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"reward\"][episode_idx][agent]\n",
    "\n",
    "        ax.plot(reward)\n",
    "        ax.set_title(f\"Reward per step ({agent = })\")\n",
    "        ax.set_ylabel(\"Reward\")\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        # ax.yaxis.set_major_locator(ticker.MultipleLocator(0.2))\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)\n",
    "\n",
    "\n",
    "make_reward_plot(raw_exp_data, env, iteration_idx, episode_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Input rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_rate_plot(raw_exp_data, env, iteration_idx, episode_idx):\n",
    "    for agent in [\"all\"] + env.agents:\n",
    "        plt.close(fig=f\"input_rate_{agent}\")\n",
    "        fig = plt.figure(num=f\"input_rate_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        if agent == \"all\":\n",
    "            input_rate = np.add(\n",
    "                *[\n",
    "                    raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"observation_input_rate\"][episode_idx][\n",
    "                        agent\n",
    "                    ]\n",
    "                    for agent in env.agents\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            input_rate = raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"observation_input_rate\"][\n",
    "                episode_idx\n",
    "            ][agent]\n",
    "\n",
    "        ax.plot(input_rate)\n",
    "        ax.set_title(f\"Input rate per step ({agent = })\")\n",
    "        ax.set_ylabel(\"Input rate\")\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(25))\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)\n",
    "\n",
    "\n",
    "make_input_rate_plot(raw_exp_data, env, iteration_idx, episode_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Absolute Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absolute_action(raw_exp_data, env, iteration_idx, episode_idx):\n",
    "    data = {}\n",
    "    for agent in env.agents:\n",
    "        data[agent] = {}\n",
    "        data[agent][\"input_rate\"] = np.array(\n",
    "            raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"observation_input_rate\"][episode_idx][agent]\n",
    "        )\n",
    "        data[agent][\"action_local\"] = np.array(\n",
    "            raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"action_local\"][episode_idx][agent]\n",
    "        )\n",
    "        data[agent][\"action_forward\"] = np.array(\n",
    "            raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"action_forward\"][episode_idx][agent]\n",
    "        )\n",
    "        data[agent][\"action_reject\"] = np.array(\n",
    "            raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"action_reject\"][episode_idx][agent]\n",
    "        )\n",
    "\n",
    "        forward_reject = np.array(\n",
    "            raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"forward_reject_rate\"][episode_idx][agent]\n",
    "        )\n",
    "        local_reject = np.array(\n",
    "            raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"incoming_rate_local_reject\"][episode_idx][agent]\n",
    "        )\n",
    "        data[agent][\"reject_rate\"] = forward_reject + local_reject + data[agent][\"action_reject\"]\n",
    "\n",
    "    data[\"all\"] = {}\n",
    "    data[\"all\"][\"input_rate\"] = np.add(*[data[agent][\"input_rate\"] for agent in env.agents])\n",
    "    data[\"all\"][\"action_local\"] = np.add(*[data[agent][\"action_local\"] for agent in env.agents])\n",
    "    data[\"all\"][\"action_forward\"] = np.add(*[data[agent][\"action_forward\"] for agent in env.agents])\n",
    "    data[\"all\"][\"action_reject\"] = np.add(*[data[agent][\"action_reject\"] for agent in env.agents])\n",
    "    data[\"all\"][\"reject_rate\"] = np.add(*[data[agent][\"reject_rate\"] for agent in env.agents])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "absolute_action = get_absolute_action(raw_exp_data, env, iteration_idx, episode_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_absolute_action_plot(absolute_action):\n",
    "    for agent, agent_absolute_action in sorted(absolute_action.items()):\n",
    "        plt.close(fig=f\"absolute_action_{agent}\")\n",
    "        fig = plt.figure(num=f\"absolute_action_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        steps = np.arange(len(agent_absolute_action[\"input_rate\"]))\n",
    "\n",
    "        ax.plot(agent_absolute_action[\"input_rate\"], label=\"Input rate\")\n",
    "        ax.stackplot(\n",
    "            steps,\n",
    "            agent_absolute_action[\"action_local\"],\n",
    "            agent_absolute_action[\"action_forward\"],\n",
    "            agent_absolute_action[\"action_reject\"],\n",
    "            labels=[\"Action Local\", \"Action Forward\", \"Action Reject\"],\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax.plot(agent_absolute_action[\"reject_rate\"], label=\"Reject rate\")\n",
    "\n",
    "        ax.set_title(f\"Action distribution per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Requests\")\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        # Use only integer ticks and let matplotlib to choose how many ticks to show.\n",
    "        # ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_absolute_action_plot(absolute_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_action_distribution_plot(absolute_action, env, iteration_idx, episode_idx):\n",
    "    for agent in [\"all\"] + env.agents:\n",
    "        plt.close(fig=f\"action_distribution_{agent}\")\n",
    "        fig = plt.figure(num=f\"action_distribution_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "\n",
    "    action_local_tmp, action_forward_tmp, action_reject_tmp = [], [], []\n",
    "    for agent in env.agents + [\"all\"]:\n",
    "        fig = plt.figure(num=f\"action_distribution_{agent}\")\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        steps = np.arange(env.max_steps)\n",
    "\n",
    "        if agent == \"all\":  # Always executed at the end.\n",
    "            # Sum all columns (one row for each agent).\n",
    "            action_local = np.array(action_local_tmp).sum(axis=0)\n",
    "            action_forward = np.array(action_forward_tmp).sum(axis=0)\n",
    "            action_reject = np.array(action_reject_tmp).sum(axis=0)\n",
    "        else:\n",
    "            action_local = np.array(\n",
    "                raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"action_local\"][episode_idx][agent]\n",
    "            )\n",
    "            action_forward = np.array(\n",
    "                raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"action_forward\"][episode_idx][agent]\n",
    "            )\n",
    "            action_reject = np.array(\n",
    "                raw_exp_data[iteration_idx][\"env_runners\"][\"hist_stats\"][\"action_reject\"][episode_idx][agent]\n",
    "            )\n",
    "\n",
    "            action_local_tmp.append(action_local)\n",
    "            action_forward_tmp.append(action_forward)\n",
    "            action_reject_tmp.append(action_reject)\n",
    "\n",
    "        input_rate = action_local + action_forward + action_reject\n",
    "\n",
    "        # Convert to ratios, but make sure to avoid division by zero (when input_rate is zero).\n",
    "        action_local = np.divide(action_local, input_rate, out=np.zeros(env.max_steps), where=input_rate != 0) * 100\n",
    "        action_forward = np.divide(action_forward, input_rate, out=np.zeros(env.max_steps), where=input_rate != 0) * 100\n",
    "        action_reject = np.divide(action_reject, input_rate, out=np.zeros(env.max_steps), where=input_rate != 0) * 100\n",
    "\n",
    "        ax.stackplot(\n",
    "            steps, action_local, action_forward, action_reject, labels=[\"Local\", \"Forward\", \"Reject\"], alpha=0.8\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Action distribution per step ({agent = })\")\n",
    "        ax.set_ylabel(\"Action\")\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        ax.yaxis.set_major_formatter(ticker.PercentFormatter())\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(25))\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)\n",
    "\n",
    "\n",
    "episode_idx\n",
    "\n",
    "make_action_distribution_plot(raw_exp_data, env, iteration_idx, episode_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Agent Reject Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "These plots show the rejection rate of each agent and the cumulative rejection rate of all agents as a percentage of the total input rate for each step.\n",
    "\n",
    "The agent rejection rate is the sum of the rejection rates by action, local unprocessed rates, and forwarded rates not processed by neighboring agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_agent_reject_rate_plot(absolute_action):\n",
    "    for agent, agent_absolute_action in sorted(absolute_action.items()):\n",
    "        plt.close(fig=f\"agent_reject_rate_{agent}\")\n",
    "        fig = plt.figure(num=f\"agent_reject_rate_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        if agent != \"all\":\n",
    "            reject_percentual = agent_absolute_action[\"reject_rate\"] / agent_absolute_action[\"input_rate\"] * 100\n",
    "        else:\n",
    "            # Cumulate the percentual over all agents for the \"all\" case.\n",
    "            reject_rates, input_rates = [], []\n",
    "            for cum_agent, cum_agent_action in absolute_action.items():\n",
    "                if cum_agent == \"all\":\n",
    "                    continue\n",
    "                reject_rates.append(cum_agent_action[\"reject_rate\"])\n",
    "                input_rates.append(cum_agent_action[\"input_rate\"])\n",
    "\n",
    "            # Sum across agents per step.\n",
    "            total_reject = np.sum(reject_rates, axis=0)\n",
    "            total_input = np.sum(input_rates, axis=0)\n",
    "\n",
    "            # Get cumulative rate for each step, avoiding division by zero (in this case just use 0).\n",
    "            with np.errstate(divide=\"ignore\"):\n",
    "                reject_percentual = np.where(total_input > 0, total_reject / total_input * 100, 0)\n",
    "\n",
    "        ax.plot(reject_percentual)\n",
    "\n",
    "        ax.set_title(f\"Agent reject rate¹ per step ({agent = })\")\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.2,\n",
    "            \"¹reject rate = action reject + local reject + forward reject\",\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        ax.set_ylabel(\"Reject rate\")\n",
    "        ax.yaxis.set_major_formatter(ticker.PercentFormatter())\n",
    "        ax.set_xlabel(\"Step\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "# Reuse absolute_action calculated before.\n",
    "make_agent_reject_rate_plot(absolute_action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
