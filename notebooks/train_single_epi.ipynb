{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Train summary for a single episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook plots data of a single episode of an experiment using PPO and one episode played for each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Experiment loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports.\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib widget\n",
    "import base\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import ipywidgets\n",
    "\n",
    "import dfaas_env\n",
    "import dfaas_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_dir = Path(\"/home/emanuele/marl-dfaas/results/\")\n",
    "\n",
    "exp_dir = prefix_dir / \"DFAAS-MA_2025-05-05_12-09-37_PPO_2_base\"\n",
    "\n",
    "# Raw data dictionary \"result.json\".\n",
    "raw_exp_data = dfaas_utils.parse_result_file(exp_dir / \"result.json\")\n",
    "\n",
    "# Reference environment.\n",
    "env = base.get_env(exp_dir)\n",
    "\n",
    "agents = [\"all\"] + env.agents\n",
    "\n",
    "print(f\"Loaded experiment: {exp_dir.as_posix()!r}\")\n",
    "print(f\"Agents: {agents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reward_plot(raw_exp_data, env, iter_idx, agents):\n",
    "    for agent in agents:\n",
    "        plt.close(fig=f\"reward_cum_{agent}\")\n",
    "        fig = plt.figure(num=f\"reward_cum_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        if agent == \"all\":\n",
    "            reward = np.add(\n",
    "                *[raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"reward\"][0][agent] for agent in env.agents]\n",
    "            )\n",
    "        else:\n",
    "            reward = raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"reward\"][0][agent]\n",
    "\n",
    "        ax.plot(reward)\n",
    "        ax.set_title(f\"Reward per step ({agent = })\")\n",
    "        ax.set_ylabel(\"Reward\")\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(0.2))\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)\n",
    "\n",
    "\n",
    "make_reward_plot(raw_exp_data, env, iter_idx, agents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
