{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Train statistics of a single experiment (SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook presents training statistics for the policies that have been trained with the SAC algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Experiment loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports.\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib widget\n",
    "import base\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import dfaas_env\n",
    "import dfaas_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_dir = Path(\"/home/emanuele/marl-dfaas/results/\")\n",
    "\n",
    "exp_dir = prefix_dir / \"DF_20250619_102139_SAC_initial_test_10000\"\n",
    "\n",
    "# Raw data dictionary \"result.json\".\n",
    "raw_exp_data = dfaas_utils.parse_result_file(exp_dir / \"result.json\")\n",
    "\n",
    "# Reference environment.\n",
    "env = base.get_env(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Experiment prefix dir: {prefix_dir.as_posix()!r}\")\n",
    "print(f\"Experiment name:       {exp_dir.name!r}\")\n",
    "print(f\"Agents:                {env.agents} ({len(env.agents)})\")\n",
    "print(f\"Iterations:            {len(raw_exp_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Reward statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_per_episode(raw_exp_data, env):\n",
    "    \"\"\"Returns the reward per episode for each agent and all agents.\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reward_sum = {}\n",
    "    reward_sum[\"all\"] = np.empty(iters_n)\n",
    "    for agent in env.agents:\n",
    "        reward_sum[agent] = np.empty(iters_n)\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        reward_sum[\"all\"][iter_idx] = np.average(raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"episode_reward\"])\n",
    "        for agent in env.agents:\n",
    "            reward_sum[agent][iter_idx] = np.average(\n",
    "                raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][f\"policy_policy_{agent}_reward\"]\n",
    "            )\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "stats_reward_episode = get_reward_per_episode(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reward_per_episode_plot(stats):\n",
    "    for agent, reward in stats.items():\n",
    "        plt.close(fig=f\"reward_per_episode_{agent}\")\n",
    "        fig = plt.figure(num=f\"reward_cum_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(reward)\n",
    "\n",
    "        ax.set_title(f\"Average reward per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Reward\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_reward_per_episode_plot(stats_reward_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Total Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The total loss in the SAC algorithm is the combination of three components:\n",
    "\n",
    "1. **Policy Loss**, or also called Actor Loss,\n",
    "2. **Q-function Loss**, or also called Critic Loss,\n",
    "3. **Entropy temperature loss**, or also called Alpha Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_loss(raw_exp_data):\n",
    "    \"\"\"Returns the total loss for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            try:\n",
    "                # Currently for SAC there is no \"total_loss\" field like PPO, so we\n",
    "                # must calculate it manually.\n",
    "                actor_loss = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"actor_loss\"]\n",
    "                critic_loss = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"critic_loss\"]\n",
    "                alpha_loss = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"alpha_loss\"]\n",
    "                loss[policy][iter] = actor_loss + critic_loss + alpha_loss\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                loss[policy][iter] = 0\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "total_loss = get_total_loss(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_total_loss_plot(total_loss):\n",
    "    for policy, loss in total_loss.items():\n",
    "        fig_name = f\"total_loss_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(loss)\n",
    "\n",
    "        ax.set_title(f\"Total loss for {policy}\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_total_loss_plot(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Policy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "This is the loss associated with the policy (actor) network. Also called Actor Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_loss(raw_exp_data):\n",
    "    \"\"\"Returns the policy loss for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            try:\n",
    "                loss[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"actor_loss\"]\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                loss[policy][iter] = 0\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "policy_loss = get_policy_loss(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_policy_loss_plot(policy_loss):\n",
    "    for policy, loss in policy_loss.items():\n",
    "        fig_name = f\"policy_loss_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(loss)\n",
    "\n",
    "        ax.set_title(f\"Policy loss (Actor Loss) for {policy}\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_policy_loss_plot(policy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Q-function Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "This is the loss associated to the critic network(s). Also called Critic Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic_loss(raw_exp_data):\n",
    "    \"\"\"Returns the critic loss (q-function loss) for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            try:\n",
    "                loss[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"critic_loss\"]\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                loss[policy][iter] = 0\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "critic_loss = get_critic_loss(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_critic_loss_plot(critic_loss):\n",
    "    for policy, loss in critic_loss.items():\n",
    "        fig_name = f\"critic_loss_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(loss)\n",
    "\n",
    "        ax.set_title(f\"Q-function loss (Critic Loss) for {policy}\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_critic_loss_plot(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Entropy Temperature Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "This is the loss associated with the dynamic learning of the entropy parameter alpha (to balance the trade-off between exploration and exploitation). Also called Alpha Loss. The learning is based on the target entropy, that is a predefined value.\n",
    "\n",
    "If the predefined value is \"auto\", the value is automatically set based on the action space dimensionality:\n",
    "\n",
    "* For continuous action spaces: `-dim(A)` (A is the action space),\n",
    "* For discrete action spaces: `-log(1/|A|)` (`|A|` is the number of possibile actions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_loss(raw_exp_data):\n",
    "    \"\"\"Returns the Entropy Temperature Loss (Alpha Loss) for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            try:\n",
    "                loss[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"alpha_loss\"]\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                loss[policy][iter] = 0\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "alpha_loss = get_alpha_loss(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_entropy(raw_exp_data, env):\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    target_entropy = {}\n",
    "\n",
    "    for policy in policies:\n",
    "        for iter in range(iters):\n",
    "            try:\n",
    "                target_entropy[policy] = raw_exp_data[15][\"info\"][\"learner\"][policy][\"learner_stats\"][\"target_entropy\"][\n",
    "                    0\n",
    "                ]\n",
    "                break\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "    return target_entropy\n",
    "\n",
    "\n",
    "target_entropy = get_target_entropy(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for policy in raw_exp_data[0][\"config\"][\"policies_to_train\"]:\n",
    "    print(f\"Target entropy for {policy!r}: {target_entropy[policy]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alpha_loss_plot(alpha_loss):\n",
    "    for policy, loss in alpha_loss.items():\n",
    "        fig_name = f\"alpha_loss_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(loss)\n",
    "\n",
    "        ax.set_title(f\"Entropy Temperature Loss (Alpha Loss) for {policy}\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_alpha_loss_plot(alpha_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Alpha value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Since the entropy temperature parameter alpha is automatically tuned in every training iteration, we log the value. The meaning of this parameterer is how much importance is being placed on policy entropy; higher means more exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_value(raw_exp_data):\n",
    "    \"\"\"Returns the Entropy Temperature Parameter (alpha) for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    value = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            try:\n",
    "                # There may be multiple alpha values, depending on the action space.\n",
    "                alpha_values = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"alpha_value\"]\n",
    "                assert (\n",
    "                    len(alpha_values) == 1\n",
    "                ), f\"Only one entropy temperature parameter is supported, found {len(alpha_values)}\"\n",
    "\n",
    "                value[policy][iter] = alpha_values[0]\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                value[policy][iter] = 0\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "alpha_value = get_alpha_value(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alpha_value_plot(alpha_value):\n",
    "    for policy, value in alpha_value.items():\n",
    "        fig_name = f\"alpha_value_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(value)\n",
    "\n",
    "        ax.set_title(f\"Entropy Temperature Parameter (alpha) for {policy}\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_alpha_value_plot(alpha_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Q-value Mean/Max/Min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Average/Min/Max Q-values (expected returns) predicted by the critic network(s) for the sampled batch of data in each training iteration.\n",
    "\n",
    "The average Q-value should increase over time, this means the agent is learning higher rewards. The min/max shows the spread (variance) of the estimates. In the long term, the Q-values should not go to infinity (diverge) or become stuck at a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_value_stats(raw_exp_data):\n",
    "    \"\"\"Returns the Mean/Max/Min Q-values for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    q_value = {}\n",
    "    for policy in policies:\n",
    "        q_value[policy] = {stat: np.zeros(iters) for stat in [\"max\", \"min\", \"mean\"]}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            try:\n",
    "                q_value[policy][\"mean\"][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"mean_q\"]\n",
    "                q_value[policy][\"min\"][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"min_q\"]\n",
    "                q_value[policy][\"max\"][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"max_q\"]\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                continue\n",
    "\n",
    "    return q_value\n",
    "\n",
    "\n",
    "q_value = get_q_value_stats(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_q_value_plot(q_value):\n",
    "    for policy, policy_q_value in q_value.items():\n",
    "        fig_name = f\"q_value_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        default_first_color = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"][0]\n",
    "        iterations = np.arange(len(policy_q_value[\"mean\"]))\n",
    "\n",
    "        ax.plot(policy_q_value[\"mean\"], label=\"Mean\", color=default_first_color)\n",
    "        ax.fill_between(\n",
    "            iterations,\n",
    "            policy_q_value[\"min\"],\n",
    "            policy_q_value[\"max\"],\n",
    "            color=default_first_color,\n",
    "            alpha=0.4,\n",
    "            label=\"Min/Max\",\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Predicted Q-values for {policy}\")\n",
    "        ax.set_ylabel(\"Reward\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_q_value_plot(q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Mean/Dist TD Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "It is the average of **Temporal Difference** (TD) errors of the critic network(s) over the current training batch. The single TD error is how much the predicted Q-value differs from the \"target\" value computed using the observed reward and the target network.\n",
    "\n",
    "* A large average TD error (positive or negative) can signal instability, underfitting, or divergence.\n",
    "* A large average TD error near zero means the critic's predictions are close to the targets, which is typically desired as training progresses.\n",
    "\n",
    "It should generally decrease and stabilize (but may fluctuate) as the agent learns.\n",
    "\n",
    "There is a plot also for the distribution of the TD errors, since Ray RLLib logs them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_td_error(raw_exp_data):\n",
    "    \"\"\"Returns the average of Temporal Difference errors for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    mean_td_error = {policy: np.zeros(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            try:\n",
    "                mean_td_error[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"mean_td_error\"]\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                continue\n",
    "\n",
    "    return mean_td_error\n",
    "\n",
    "\n",
    "mean_td_error = get_mean_td_error(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_td_errors(raw_exp_data):\n",
    "    \"\"\"Returns the Temporal Difference errors for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    td_errors = {}\n",
    "\n",
    "    # We need first to get the number of errors per iteration, we assume it is the\n",
    "    # same for all training iterations.\n",
    "    for policy in policies:\n",
    "        td_errors_size = 0\n",
    "        for iter in range(iters):\n",
    "            try:\n",
    "                td_errors_size = len(raw_exp_data[iter][\"info\"][\"learner\"][policy][\"td_error\"])\n",
    "                break\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                continue\n",
    "\n",
    "        assert td_errors_size > 0\n",
    "        td_errors[policy] = np.zeros(shape=(iters, td_errors_size))\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            try:\n",
    "                td_errors[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"td_error\"]\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                continue\n",
    "\n",
    "    return td_errors\n",
    "\n",
    "\n",
    "td_errors = get_td_errors(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mean_td_error_plot(mean_td_error):\n",
    "    for policy, policy_mean_td_error in mean_td_error.items():\n",
    "        fig_name = f\"mean_td_error_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(policy_mean_td_error)\n",
    "\n",
    "        ax.set_title(f\"Average TD error for {policy}\")\n",
    "        ax.set_ylabel(\"Error\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_mean_td_error_plot(mean_td_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_td_errors_plot(td_errors):\n",
    "    for policy, policy_td_errors in td_errors.items():\n",
    "        fig_name = f\"td_errors_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        # Important: the array must be transposed!\n",
    "        ax.violinplot(policy_td_errors.T, showmedians=True, bw_method=0.8)\n",
    "\n",
    "        ax.set_title(f\"TD error for {policy}\")\n",
    "        ax.set_ylabel(\"Error\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_td_errors_plot(td_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Gradient and Optimization Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Global Gradient Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "It is the Euclidean norm of the gradients computed during a single optimisation step. It measures the size of the policy/value gradients during learning.\n",
    "\n",
    "* A very large value can indicate unstable learning or exploding gradients.\n",
    "* A very small value (close to zero) means the model's parameters are barely changing (possibly due to vanishing gradients or convergence).\n",
    "\n",
    "The scale of the value is influenced by the network's structure and the distribution of actions. Fluctuations are normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_gnorm(raw_exp_data):\n",
    "    \"\"\"Returns the global Gradient Norm for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            try:\n",
    "                loss[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"grad_gnorm\"]\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                loss[policy][iter] = 0\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "grad_gnorm = get_grad_gnorm(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grad_gnorm_plot(grad_gnorm):\n",
    "    for policy, policy_grad_gnorm in grad_gnorm.items():\n",
    "        fig_name = f\"grad_gnorm_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(policy_grad_gnorm)\n",
    "\n",
    "        ax.set_title(f\"Global Gradient Norm for {policy}\")\n",
    "        ax.set_ylabel(\"Global Gradient Norm\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_grad_gnorm_plot(grad_gnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Gradient Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "The total number of gradient updates (optimizer steps) performed by the learner for this policy since the beginning of training. Since in SAC the number of SGD passes per batch can be changed over time, can be helpful to see the increasing curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_updates(raw_exp_data):\n",
    "    \"\"\"Returns the Gradient Updates count for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    updates = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            try:\n",
    "                updates[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"num_grad_updates_lifetime\"]\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                updates[policy][iter] = 0\n",
    "\n",
    "    return updates\n",
    "\n",
    "\n",
    "grad_updates = get_grad_updates(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grad_updates_plot(grad_updates):\n",
    "    for policy, policy_grad_updates in grad_updates.items():\n",
    "        fig_name = f\"grad_updates_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(policy_grad_updates)\n",
    "\n",
    "        ax.set_title(f\"Cumulative gradient updates since beginning for {policy}\")\n",
    "        ax.set_ylabel(\"Updates\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_grad_updates_plot(grad_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Diff. Gradient Updates over Sample Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "It is the difference between the number of gradient updates that have been performed and the number of environment timesteps collected for policy sampling.\n",
    "\n",
    "WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_grad_updates(raw_exp_data):\n",
    "    \"\"\"Returns the difference of Gradient Updates over Sample Policy for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"config\"][\"policies_to_train\"]\n",
    "\n",
    "    diff = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            try:\n",
    "                diff[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\n",
    "                    \"diff_num_grad_updates_vs_sampler_policy\"\n",
    "                ]\n",
    "            except KeyError:\n",
    "                # No training done for this iteration (maybe the replay buffer\n",
    "                # is not yet full).\n",
    "                diff[policy][iter] = 0\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "diff_grad = get_diff_grad_updates(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_diff_grad_updates_plot(diff_grad):\n",
    "    for policy, policy_diff_grad in diff_grad.items():\n",
    "        fig_name = f\"diff_grad_updates_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(policy_diff_grad)\n",
    "\n",
    "        ax.set_title(f\"Difference in Updates vs. Samples for {policy}\")\n",
    "        ax.set_ylabel(\"Difference\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_diff_grad_updates_plot(diff_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
