{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Neural Networks used in PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "**Important**: the following is valid only for Ray 2.40.0!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "I use PPO to perform experiments with the DFaaS environment. Since I have adopted a decentralized training approach for this multi-agent problem, each trained agent is associated with a policy that has two artificial neural networks, one for the actor and one for the critic. This notebook explores the structure of these networks.\n",
    "\n",
    "Main official page documentation: [Models, Preprocessors, and Action Distributions](https://docs.ray.io/en/releases-2.40.0/rllib/rllib-models.html#models-preprocessors-and-action-distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "If no option is given, the default neural network used is a fully connected network. This network is configurable and the defaults are stored in the `MODEL_DEFAULT` dictionary [rllib/models/catalog.py](https://github.com/ray-project/ray/blob/887eddd9245c77adc5684c78410400327d266427/rllib/models/catalog.py#L52)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The fully connected network in Ray is represented by the `FullyConnectedNetwork` class in [rllib/models/torch/fcnet.py](https://github.com/ray-project/ray/blob/releases/2.40.0/rllib/models/torch/fcnet.py).\n",
    "\n",
    "With the default options:\n",
    "\n",
    "* Two hidden layers with 256 neurons each,\n",
    "* [tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html) as the activation function for the hidden layers,\n",
    "* Linear activation function for the output layer.\n",
    "\n",
    "This is the architecture of the Critic network, but the Actor network shares the same architecture (with different inputs and outputs, of course).\n",
    "\n",
    "In the `FullyConnectedNetwork` class, the neural networks are stored in the following object variables:\n",
    "\n",
    "* `_logits`: is the output layer of the actor network;\n",
    "* `_hidden_layers`: contains the hidden layers of the actor network;\n",
    "* `_value_branch_separate`: contains the hidden layers of the critic network;\n",
    "* `_value_branch`: is the output layer of the critic network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Default neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports.\n",
    "from pathlib import Path\n",
    "from pprint import pformat as pf\n",
    "from pprint import pp\n",
    "\n",
    "import base\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.models.catalog import MODEL_DEFAULTS\n",
    "\n",
    "import dfaas_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy environment, used to get observation and action spaces.\n",
    "dummy_env = dfaas_env.DFaaS()\n",
    "dummy_agent = dummy_env.agents[0]\n",
    "\n",
    "# Normally we would have one policy for each agent, but in this simplified\n",
    "# version we only need to show the network architecture, so one policy for\n",
    "# all agents is sufficient.\n",
    "policies = {\n",
    "    \"policy_node_x\": PolicySpec(\n",
    "        policy_class=None,\n",
    "        observation_space=dummy_env.observation_space[dummy_agent],\n",
    "        action_space=dummy_env.action_space[dummy_agent],\n",
    "        config=None,\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "# Link the single policy to all agents.\n",
    "def policy_mapping_fn(agent_id, episode, runner, **kwargs):\n",
    "    return \"policy_node_x\"\n",
    "\n",
    "\n",
    "# Algorithm config.\n",
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    # By default RLlib uses the new API stack, but I use the old one.\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False\n",
    "    )\n",
    "    .environment(env=dfaas_env.DFaaS.__name__)\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=0)\n",
    "    .evaluation(evaluation_interval=None)\n",
    "    .resources(num_gpus=1)\n",
    "    .callbacks(dfaas_env.DFaaSCallbacks)\n",
    "    .multi_agent(policies=policies, policy_mapping_fn=policy_mapping_fn)\n",
    ")\n",
    "\n",
    "# Build the algorithm.\n",
    "ppo_algo = ppo_config.build()\n",
    "\n",
    "# Get the (only) policy.\n",
    "policy = ppo_algo.get_policy(\"policy_node_x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_observation_space = dummy_env.observation_space[dummy_agent]\n",
    "agent_action_space = dummy_env.action_space[dummy_agent]\n",
    "\n",
    "print(f\"Agent observaton space = {pf(dict(agent_observation_space))}\\n\")\n",
    "print(\"Agent action space =\", agent_action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Policy observation space = {policy.observation_space}\\n\")\n",
    "print(\n",
    "    f\"Policy original observation space = {pf(dict(policy.observation_space.original_space))}\\n\"\n",
    ")\n",
    "\n",
    "print(f\"Policy Action space = {policy.action_space}\\n\")\n",
    "\n",
    "print(f\"Model\\n{policy.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "**Note**: The input to the networks is pre-processed, which is why the policy's observation space and the agent's observation space are different. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "This default neural network is not suitable for the DFaaS environment for two reasons:\n",
    "\n",
    "1. We have a discrete (finite) observation space and a continuous action space (the concentration parameters): more hidden layers allow the agent to learn complex patterns when distributing requests.\n",
    "\n",
    "2. The network output (actor only) is a vector of concentration parameters for Dirichlet distribution. These parameters are strictly positive, but since there is a linear activation function on the output layer (`_logits`), we can get negative parameters. The [Softplus](https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html#torch.nn.Softplus) activation function should be more appropriate.\n",
    "\n",
    "3. The same is true for the critic network, since the reward function returns non-negative rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Updated neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The model can be customized using the `custom_model_config` subdictionary. There is a key `last_activation_fn` that specifies the activation function for the last output layer of the actor network.\n",
    "\n",
    "Note that I have modified the `FullyConnectedNetwork` class to support this new functionality, since the original class forces the linear function as the activation function.\n",
    "\n",
    "The following code specify a model that uses the Softplus function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy environment, used to get observation and action spaces.\n",
    "dummy_env = dfaas_env.DFaaS()\n",
    "dummy_agent = dummy_env.agents[0]\n",
    "\n",
    "# Normally we would have one policy for each agent, but in this simplified\n",
    "# version we only need to show the network architecture, so one policy for\n",
    "# all agents is sufficient.\n",
    "policies = {\n",
    "    \"policy_node_x\": PolicySpec(\n",
    "        policy_class=None,\n",
    "        observation_space=dummy_env.observation_space[dummy_agent],\n",
    "        action_space=dummy_env.action_space[dummy_agent],\n",
    "        config=None,\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "# Link the single policy to all agents.\n",
    "def policy_mapping_fn(agent_id, episode, runner, **kwargs):\n",
    "    return \"policy_node_x\"\n",
    "\n",
    "\n",
    "# Customize the default model.\n",
    "model = MODEL_DEFAULTS.copy()\n",
    "model[\"custom_model_config\"] = {\"last_activation_fn\": \"Softplus\"}\n",
    "model[\"vf_share_layers\"] = False\n",
    "\n",
    "# Algorithm config.\n",
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    # By default RLlib uses the new API stack, but I use the old one.\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False\n",
    "    )\n",
    "    .environment(env=dfaas_env.DFaaS.__name__)\n",
    "    .training(model=model)\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=0)\n",
    "    .evaluation(evaluation_interval=None)\n",
    "    .resources(num_gpus=1)\n",
    "    .callbacks(dfaas_env.DFaaSCallbacks)\n",
    "    .multi_agent(policies=policies, policy_mapping_fn=policy_mapping_fn)\n",
    ")\n",
    "\n",
    "# Build the algorithm.\n",
    "ppo_algo = ppo_config.build()\n",
    "\n",
    "# Get the (only) policy.\n",
    "policy = ppo_algo.get_policy(\"policy_node_x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model\\n{policy.model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
