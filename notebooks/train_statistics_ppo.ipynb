{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Train statistics of a single experiment (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook presents training statistics for the policies that have been trained with the PPO algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Experiment loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports.\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib widget\n",
    "import base\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import networkx as nx\n",
    "\n",
    "import dfaas_env\n",
    "import dfaas_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory where the experiments are located.\n",
    "prefix_dir = Path(\"../results/\").resolve().absolute()\n",
    "\n",
    "# Experiment directory.\n",
    "exp_dir = prefix_dir / \"DF_20250929_144721_PPO_5_agents_expanded_actions\"\n",
    "\n",
    "# Alternative set an absolute directory. Not suggested.\n",
    "# exp_dir = Path(\"/home/emanuele/marl-dfaas/results/DF_20250929_144725_PPO_5_agents\")\n",
    "# prefix_dir = exp_dir.parent\n",
    "\n",
    "# Raw data dictionary \"result.json\".\n",
    "raw_exp_data = dfaas_utils.parse_result_file(exp_dir / \"result.json\")\n",
    "\n",
    "# Reference environment.\n",
    "env = base.get_env(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Experiment prefix dir: {prefix_dir.as_posix()!r}\")\n",
    "print(f\"Experiment name:       {exp_dir.name!r}\")\n",
    "print(f\"Agents:                {env.agents} ({len(env.agents)})\")\n",
    "print(f\"Iterations:            {len(raw_exp_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_networkx_plot(graph):\n",
    "    plt.close(fig=f\"networkx\")\n",
    "    fig = plt.figure(num=f\"networkx\", layout=\"constrained\")\n",
    "    fig.canvas.header_visible = False\n",
    "    ax = fig.subplots()\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    options = {\n",
    "        \"ax\": ax,\n",
    "        \"node_size\": 2500,\n",
    "        \"node_color\": \"white\",\n",
    "        \"edgecolors\": \"black\",\n",
    "    }\n",
    "\n",
    "    nx.draw_networkx(graph, **options)\n",
    "    ax.set_title(f\"Network topology\")\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "\n",
    "make_networkx_plot(env.network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Reward statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Reward per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_per_episode(raw_exp_data, env):\n",
    "    \"\"\"Returns the reward per episode for each agent and all agents.\"\"\"\n",
    "    iters_n = len(raw_exp_data)\n",
    "\n",
    "    reward_sum = {}\n",
    "    reward_sum[\"all\"] = np.empty(iters_n)\n",
    "    for agent in env.agents:\n",
    "        reward_sum[agent] = np.empty(iters_n)\n",
    "\n",
    "    for iter_idx in range(len(raw_exp_data)):\n",
    "        reward_sum[\"all\"][iter_idx] = np.average(raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][\"episode_reward\"])\n",
    "        for agent in env.agents:\n",
    "            reward_sum[agent][iter_idx] = np.average(\n",
    "                raw_exp_data[iter_idx][\"env_runners\"][\"hist_stats\"][f\"policy_policy_{agent}_reward\"]\n",
    "            )\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "stats_reward_episode = get_reward_per_episode(raw_exp_data, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reward_per_episode_plot(stats):\n",
    "    for agent, reward in stats.items():\n",
    "        plt.close(fig=f\"reward_per_episode_{agent}\")\n",
    "        fig = plt.figure(num=f\"reward_cum_{agent}\", layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(reward)\n",
    "\n",
    "        ax.set_title(f\"Average reward per episode ({agent = })\")\n",
    "        ax.set_ylabel(\"Reward\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_reward_per_episode_plot(stats_reward_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Policy/Training Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Total Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The overall loss is used to update the policy network in a single gradient step. It is a combination of:\n",
    "\n",
    "* Policy loss\n",
    "* Value function loss\n",
    "* Differential Entropy\n",
    "* KL divergence penalty\n",
    "\n",
    "The range depends on the reward scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_loss(raw_exp_data):\n",
    "    \"\"\"Returns the total loss for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"info\"][\"learner\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            loss[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"total_loss\"]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "total_loss = get_total_loss(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_total_loss_plot(total_loss):\n",
    "    for policy, loss in total_loss.items():\n",
    "        fig_name = f\"total_loss_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(loss)\n",
    "\n",
    "        ax.set_title(f\"Total loss for {policy}\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_total_loss_plot(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Policy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "This is the loss associated with the policy (actor) network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_loss(raw_exp_data):\n",
    "    \"\"\"Returns the policy loss for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"info\"][\"learner\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            loss[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"policy_loss\"]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "policy_loss = get_policy_loss(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_policy_loss_plot(policy_loss):\n",
    "    for policy, loss in policy_loss.items():\n",
    "        fig_name = f\"policy_loss_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(loss)\n",
    "\n",
    "        ax.set_title(f\"Policy loss for {policy}\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_policy_loss_plot(policy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Value Function Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "This is the loss associated with the value (critic) network. It measures how closely the predictions of the value network match the actual returns observed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_function_loss(raw_exp_data):\n",
    "    \"\"\"Returns the value function loss for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"info\"][\"learner\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            loss[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"vf_loss\"]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "value_loss = get_value_function_loss(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_value_loss_plot(value_loss):\n",
    "    for policy, loss in value_loss.items():\n",
    "        fig_name = f\"value_loss_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(loss)\n",
    "\n",
    "        ax.set_title(f\"Value loss for {policy}\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_value_loss_plot(value_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Value Function Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "This is a normalised measure of how well the value function's predictions explain the variation in actual returns. The typical range is from negative to 1. Values should be closer to 1.\n",
    "\n",
    "* 1: perfect prediction.\n",
    "* 0: the predictions are no better than the mean of the targets.\n",
    "* <0: predictions are worse than just using the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_explained_var(raw_exp_data):\n",
    "    \"\"\"Returns the value function explained variance for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"info\"][\"learner\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            loss[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"vf_explained_var\"]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "value_explained_var = get_value_explained_var(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_value_explained_var_plot(value_explained_var):\n",
    "    for policy, loss in value_explained_var.items():\n",
    "        fig_name = f\"value_explained_var_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(loss)\n",
    "\n",
    "        ax.set_title(f\"Value Function Explained Variance for {policy}\")\n",
    "        ax.set_ylabel(\"Variance\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_value_explained_var_plot(value_explained_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Differential Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "**Warning**: in the context of a continuous distribution like [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution#Entropy), the entropy is actually the [differential entropy](https://en.wikipedia.org/wiki/Differential_entropy). The differential entropy [is not](https://github.com/pytorch/pytorch/issues/152845#issuecomment-2860403912) the level of exploration like for discrete probabilities distributions.\n",
    "\n",
    "The differential entropy of the policy measures information in terms of probability density. The output of the actor is the concentration parameters for a Dirichlet distribution, and the final action is sampled from this distribution.\n",
    "\n",
    "* **Very negative values** indicate that the density is high in a small volume, meaning the **distribution is highly concentrated**.\n",
    "* **Values closer to zero** or positive indicate a more diffuse distribution, meaning the **density is lower but spread over a larger volume**.\n",
    "\n",
    "Also:\n",
    "\n",
    "* If one or more parameters are small, the distribution is concentrated near the corners of the simplex, so the differential entropy is negative.\n",
    "* If one or more parameters are larger, the distribution spreads toward the center of the simplex, and the differential entropy increases, meaning the distribution is more spread out.\n",
    "\n",
    "Note that the plot shows the average differential entropy for each training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(raw_exp_data):\n",
    "    \"\"\"Returns the entropy for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"info\"][\"learner\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            loss[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"entropy\"]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "entropy = get_entropy(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_entropy_plot(entropy):\n",
    "    for policy, policy_entropy in entropy.items():\n",
    "        fig_name = f\"value_explained_var_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(policy_entropy)\n",
    "\n",
    "        ax.set_title(f\"Entropy for {policy}\")\n",
    "        ax.set_ylabel(\"Entropy\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_entropy_plot(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "The KL divergence measures how much the new policy has changed compared to the old policy.\n",
    "\n",
    "* A higher mean KL means the policy is changing a lot in one update (possibly too much).\n",
    "* A lower mean KL means the policy is not changing much (possibly learning too slowly).\n",
    "\n",
    "It is a non-negative metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_divergence(raw_exp_data):\n",
    "    \"\"\"Returns the KL divergence for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"info\"][\"learner\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            loss[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"kl\"]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "kl_divergence = get_kl_divergence(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kl_divergence_plot(kl_divergence):\n",
    "    for policy, policy_kl_divergence in kl_divergence.items():\n",
    "        fig_name = f\"kl_divergence_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(policy_kl_divergence)\n",
    "\n",
    "        ax.set_title(f\"KL divergence for {policy}\")\n",
    "        ax.set_ylabel(\"KL divergence\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_kl_divergence_plot(kl_divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Global Gradient Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "It is the Euclidean norm of the gradients computed during a single optimisation step. It measures the size of the policy/value gradients during learning.\n",
    "\n",
    "* A very large value can indicate unstable learning or exploding gradients.\n",
    "* A very small value (close to zero) means the model's parameters are barely changing (possibly due to vanishing gradients or convergence).\n",
    "\n",
    "The scale of the value is influenced by the network's structure and the distribution of actions. Fluctuations are normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_gnorm(raw_exp_data):\n",
    "    \"\"\"Returns the global Gradient Norm for each policy and for each training iteration.\"\"\"\n",
    "    iters = len(raw_exp_data)\n",
    "    policies = raw_exp_data[0][\"info\"][\"learner\"]\n",
    "\n",
    "    loss = {policy: np.empty(iters) for policy in policies}\n",
    "\n",
    "    for iter in range(iters):\n",
    "        for policy in policies:\n",
    "            loss[policy][iter] = raw_exp_data[iter][\"info\"][\"learner\"][policy][\"learner_stats\"][\"grad_gnorm\"]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "grad_gnorm = get_grad_gnorm(raw_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grad_gnorm_plot(grad_gnorm):\n",
    "    for policy, policy_grad_gnorm in grad_gnorm.items():\n",
    "        fig_name = f\"grad_gnorm_{policy}\"\n",
    "        plt.close(fig=fig_name)\n",
    "        fig = plt.figure(num=fig_name, layout=\"constrained\")\n",
    "        fig.canvas.header_visible = False\n",
    "        ax = fig.subplots()\n",
    "\n",
    "        ax.plot(policy_grad_gnorm)\n",
    "\n",
    "        ax.set_title(f\"Global Gradient Norm for {policy}\")\n",
    "        ax.set_ylabel(\"Global Gradient Norm\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "        ax.grid(axis=\"both\")\n",
    "        ax.set_axisbelow(True)  # By default the axis is over the content.\n",
    "\n",
    "\n",
    "make_grad_gnorm_plot(grad_gnorm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
